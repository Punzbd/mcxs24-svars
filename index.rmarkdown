---
title: "The Impact of Monetary Policy on the Real Economy and the Stock Market: The Case of Australia"
author: "Pun Suparattanapinan"

execute:
  echo: false
  
bibliography: references.bib
---


> **Abstract.** The purpose of this study is to investigate the relationship and impact of monetary policy on the real economy in Australia. The author uses quarterly data spanning from 1990 Q1 to 2023 Q4. The Structural Vector Autoregression (SVAR) model is used to capture the structural relationship, including analyzing the impulse responses of the variable of interest to structural shock. 
>
> **Keywords.** Structural VAR, SVAR, monetary policy, stock market, economic impact, impulse response function, Bayesian, cash rate

# Introduction

This study aims to explore the dynamic impact of monetary policy on the real economy in Australia, specifically focusing on economic growth, prices, and employment. How does the real economy respond? What is the magnitude and the duration of change? The stock market price is included and can be considered a leading indicator of the real economy.

According to Economic theory, the transmission mechanism of monetary policy to the real economy takes time, as it does not have a fully immediate effect, but it involves some delays. @BrischettoVoss1999 find that the contractionary of monetary policy in Australia leads to a decrease in output level between 5 and 15 quarters after the contraction. The price level also gradually falls with some delay with maximum effect, and there is an indication that the effect tends to be permanent. @Mojon2001 also investigate the effect of monetary policy across 10 euro area countries and find that a contractionary monetary policy shock leads to a temporary fall in GDP that peaks typically around four quarters after the shock and a gradual decrease in the price level.

On the other hand, @Bjornland2009 suggest that, due to the availability of information in financial market, the monetary policy and stock market have simultaneous effects. They also find that real stock prices immediately fall by seven to nine percent due to a monetary policy shock that raises the federal funds rate by 100 basis points. Similarly, @Ioannidis2008 find that the majority of OECD countries under study, periods of tight money are associated with contemporaneous declines in stock market value and also decreases expected stock returns. Additionally, according to @Bjornland2009, the changes in asset price, particularly stock price, ultimately impact the economic growth by increasing household spending based on their wealth and encouraging investment through the Tobin Q effect. The asset prices also play a role in firmâ€™s ability to fund operations through credit channel. Hence, policymakers are motivated to track the asset price as short-run indicators.

Understanding insights of these dynamic impacts within the macroeconomy, including the magnitude and duration of impacts, enables the policymakers to precisely predict the outcomes on real economy that might occur after changing the new cash target rate.


# Data

The variables in this study include the cash rate target **_(cashrate)_**, which indicates the monetary policy. GDP (in real terms), CPI, and the unemployment rate **_(unemprate)_** are included to measure the real economy. For the stock market, the author uses the All Ordinaries index (AORD) as a representative. The AORD is the market-weighted index and includes about 500 companies from the Australian Stock Exchange. Finally, total government expenditure is included to control for fiscal policy, which might also affect the economy besides monetary policy. The data is collected from the Reserve Bank of Australia (RBA), Australian Bureau of Statistics (ABS), and Yahoo Finance. The dataset spans from 1990 Q1 to 2023 Q4, comprising 136 observations. [Figure 1: time series plots (raw data)] represents the raw data time series of these 6 variables.

<br>



```{r download the data}
#| message: false
#| warning: false

  # Unemployment rate (02/1978 - 02/2024) monthly data
unemp_rate_raw <- readrba::read_rba(series_id = "GLFSURSA")
unemp_rate     <- unemp_rate_raw[, c("date", "value")]
unemp_rate     <- xts::xts(unemp_rate$value,unemp_rate$date)
    # we fix the period (1990Q1 - 2023Q4)    
unemp_rate     <- xts::to.quarterly(unemp_rate, OHLC = FALSE)
unemprate      <- unemp_rate[zoo::index(unemp_rate) >= "1990 Q1" & zoo::index(unemp_rate) < "2024 Q1"]

  # GDP deflator (09/1959 - 12/2023) quarterly
gdp_df_raw     <- readabs::read_abs(series_id = "A2303730T")
gdp_df         <- gdp_df_raw[, c("date", "value")]
gdp_df$quarter <- zoo::as.yearqtr(gdp_df$date)
gdp_df         <- xts::xts(gdp_df$value,gdp_df$quarter)
    # we fix the period (1990Q1 - 2023Q4)  
gdp_df         <- gdp_df[zoo::index(gdp_df) >= "1989 Q4" & zoo::index(gdp_df) < "2024 Q1"]

  # Real GDP seasonal adjusted
real_gdp_raw   <- readrba::read_rba(series_id = "GGDPCVGDP")
real_gdp       <- real_gdp_raw[, c("date", "value")]
real_gdp$quarter <- zoo::as.yearqtr(real_gdp$date)
real_gdp       <- xts::xts(real_gdp$value,real_gdp$quarter)
    # we fix the period (1990Q1 - 2023Q4)   
realgdp        <- real_gdp[zoo::index(real_gdp) >= "1990 Q1" & zoo::index(real_gdp) < "2024 Q1"]
realgdp        <- realgdp/1000
lnrealgdp      <- log(realgdp)

  # Cash target rate (01/1990 - 03/2024) daily
i_raw          <- readrba::read_rba(series_id = "FIRMMCRTD")
i              <- i_raw[, c("date", "value")]
i              <- xts::xts(i$value,i$date)
    # Convert to quarter and fix the period (1990Q1 - 2023Q4)
cashrate       <- xts::to.quarterly(i, OHLC = FALSE)[1:136]


  # CPI (06/1922 - 12/2023) quarterly
CPI_raw        <- readrba::read_rba(series_id = "GCPIAG")
CPI            <- CPI_raw[, c("date", "value")]
CPI$quarter    <- zoo::as.yearqtr(CPI$date)
CPI            <- xts::xts(CPI$value,CPI$quarter)
    # we fix the period (1989Q4 - 2023Q4)
CPI            <- CPI[zoo::index(CPI) >= "1990 Q1" & zoo::index(CPI) < "2024 Q1"]
lnCPI          <- log(CPI)

  # Stock market (01/1985 - 03/2024) monthly
link_AORD <- "https://query1.finance.yahoo.com/v7/finance/download/%5EAORD?period1=460339200&period2=1711843200&interval=1mo&events=history&includeAdjustedClose=true"
AORD_raw       <- read.csv(link_AORD)
AORD           <- AORD_raw[, c("Date", "Close")]
AORD           <- xts::xts(AORD$Close,as.Date(AORD$Date))
    # Convert to quarter and fix the period (1990Q1 - 2023Q4) and convert to ln term
AORD           <- xts::to.quarterly(AORD, OHLC = FALSE)
stockprice     <- AORD[zoo::index(AORD) >= "1990 Q1" & zoo::index(AORD) < "2024 Q1" ]
lnstockprice   <- log(stockprice)

  # Government spending (Final consumption national + state and local + seasonal adj) (09/1959 - 12/2023) quarterly
gov_raw        <- readabs::read_abs(series_id = "A2304036K")
gov            <- gov_raw[, c("date", "value")]
gov$quarter    <- zoo::as.yearqtr(gov$date)
gov            <- xts::xts(gov$value,gov$quarter)
    # we fix the period (1990Q1 - 2023Q4) and convert to real term, ln term
gov_exp        <- gov[zoo::index(gov) >= "1990 Q1" & zoo::index(gov) < "2024 Q1"]
ln_gov_exp     <- log(gov_exp)
```

```{r plotting the raw data}

time <- seq(as.Date("1990-01-01"), by = "quarter", length.out = 136)
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))
par(mar=c(3,3,2,2))

plot(time, realgdp, type = "l", lwd = 2 ,col = "grey27", main = "Real GDP ($billion)")
plot(time, cashrate, type = "l", lwd = 2 ,col = "grey27", main = "Cash rate target (%)")
plot(time, CPI, type = "l", lwd = 2 ,col = "grey27", main = "CPI")
plot(time, unemprate, type = "l", lwd = 2 ,col = "grey27", main = "Unemployment rate (%)")
plot(time, stockprice, type = "l", lwd = 2 , col = "grey27", main = "Stock market index (All ordinaries)")
plot(time, gov_exp, type = "l", lwd = 2 ,col = "grey27", main = " Total government expenditure ($ billion)")
```

<div style="text-align: center;">  
##### Figure 1: time series plots (raw data) 
</div>

<br>

The author transforms the data in manner that aligns with the purpose of study. The total government expenditure are adjusted to real terms using the GDP deflator. After that, the author transforms 4 variables which are real GDP, AORD, CPI, and real government expenditure in logarithms term, denoted by **_realgdp_**, **_stockprice_**, **_CPI_**, and **_govexp_** respectively. [Figure 2: time series plots (transformed data)] represents the transformed data, which will be analyzed further.

<br>


```{r Plotting the transformed data}

time <- seq(as.Date("1990-01-01"), by = "quarter", length.out = 136)
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))

options(repr.plot.width=20, repr.plot.height=50)
par(mar=c(3,3,2,2))

plot(time, lnrealgdp, type = "l", lwd = 2 ,col = "grey27", main = "ln Real GDP")
plot(time, cashrate, type = "l", lwd = 2 ,col = "grey27", main = "Cash rate target (%)")
plot(time, lnCPI, type = "l", lwd = 2 ,col = "grey27", main = "ln CPI")
plot(time, unemprate, type = "l", lwd = 2 ,col = "grey27", main = "Unemployment rate (%)")
plot(time, lnstockprice, type = "l", lwd = 2 , col = "grey27", main = "ln stock market index price")
plot(time, ln_gov_exp, type = "l", lwd = 2 ,col = "grey27", main = "ln total government expenditure")

```


<div style="text-align: center;"> 
##### Figure 2: time series plots (transformed data) 
</div>


<br>

The statistics summary of variables from 1990 Q1 to 2023 Q4 is shown in Table 1.



```{r statistical summary}
data          <- data.frame(cashrate, lnrealgdp, lnCPI, unemprate, lnstockprice, ln_gov_exp)

summary_stats <- function(x) {
  c(
    N       <- length(x),
    Mean    <- round(mean(x, na.rm = TRUE),3),
    St.Dev. <- round(sd(x, na.rm = TRUE),3),
    Min     <- round(min(x, na.rm = TRUE),3),
    Max     <- round(max(x, na.rm = TRUE),3)
  )
}

result      <- sapply(data, summary_stats)

result_df   <- as.data.frame(t(result))
colnames(result_df) <- c("N", "Mean", "St.Dev.", "Min", "Max")
rownames(result_df) <- c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")

knitr::kable(result_df, caption = "Table 1: Summary statistics")
```


# Preliminary Results
### The autocorrelation and partial autocorrelation
In this section, the autocorrelation and partial autocorrelation analyses are used to detect patterns and check the randomness of time series. [Figure 3: ACF plots] show the autocorrelation remains in all variables, even after 5 years. This indicate the strong correlation between a time series and its lagged values.


```{r ACF plot}
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))
par(mar=c(4,4,4,2))

for (i in 1:ncol(data)) {
  acf(data[, i], main = c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")[i])
}
```

<div style="text-align: center;"> 
##### Figure 3: ACF plots
</div>
<br>

For the PACF in [Figure 4: PACF plots], it is observed that there have not been any significant spikes  for all variables, except for _unemprate_. The PACF of _unemprate_ indicates significance in partial autocorrelation at first and sixteenth quarter lags; however, this might occur due to a type I error.

```{r PACF plot}
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))
par(mar=c(4,4,4,2))

for (i in 1:ncol(data)) {
  pacf(data[, i], main = c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")[i])
}
```


<div style="text-align: center;"> 
##### Figure 4: PACF plots
</div>
<br>

### The unit root test
In this section, the Augmented Dickey-Fuller Test (ADF Test) is used to test for the stationarity assumption. The null hypothesis is that the time series data has a unit root (non-stationary). The results are shown in Table 2. At the 1% significance level, the null hypothesis cannot be rejected for all variables, indicating that these variables are unit root non-stationary.


```{r ADF test}
#| echo: false
#| message: false
#| warning: false
adf_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
rownames(adf_matrix) <- c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")
colnames(adf_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in seq_along(data)) {
  result <- tseries::adf.test(data[[i]])
  
  # Store the results in the 'adf' matrix
  adf_matrix[i, 1] <- round(as.numeric(result[1]), 2)  # ADF statistic
  adf_matrix[i, 2] <- result[2]  # p-value
  adf_matrix[i, 3] <- round(as.numeric(result[4]), 2)  # critical values
}

knitr::kable(adf_matrix, caption = "Table 2: ADF test")
```


Then, take the first difference and the second difference of all variables and rerun the ADF test. The results are shown in Table 3 and Table 4. For the first difference in Table 3, at the 1% significance level, the null hypothesis can be rejected for all variables, except _CPI_ and _dgovexp_. It is reasonable to conclude that _cashrate_, _realgdp_, _unemprate_, _stockprice_ are integrated of order 1. And _CPI_ and _govexp_ are integrated of order 2, as shown in the result in Table 4.


```{r create the first difference data}
#| echo: false
#| message: false
#| warning: false

data_diff1 <- as.data.frame(lapply(colnames(data), function(x) na.omit(diff(get(x)))))

colnames(data_diff1) <- paste0("d", c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp"))

```

```{r ADF test on first diff}
#| echo: false
#| message: false
#| warning: false
adf_diff_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
rownames(adf_diff_matrix) <- colnames(data_diff1)
colnames(adf_diff_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in seq_along(data)) {
  result_diff <- tseries::adf.test(data_diff1[[i]])
  
  # Store the results in the 'adf' matrix
  adf_diff_matrix[i, 1] <- round(as.numeric(result_diff[1]), 2)  # ADF statistic
  adf_diff_matrix[i, 2] <- result_diff[2]  # p-value
  adf_diff_matrix[i, 3] <- round(as.numeric(result_diff[4]), 2)  # critical values
}

knitr::kable(adf_diff_matrix, caption = "Table 3: ADF test on the first difference")
```

```{r create the second difference data}
#| echo: false
#| message: false
#| warning: false

data_diff2 <- as.data.frame(lapply(data_diff1, function(x) diff(x)))
colnames(data_diff2) <- paste0("d", colnames(data_diff2))
```

```{r ADF test on second diff}
#| echo: false
#| message: false
#| warning: false
adf_diff_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
rownames(adf_diff_matrix) <- colnames(data_diff2)
colnames(adf_diff_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in seq_along(data)) {
  result_diff <- tseries::adf.test(data_diff2[[i]])
  
  # Store the results in the 'adf' matrix
  adf_diff_matrix[i, 1] <- round(as.numeric(result_diff[1]), 2)  # ADF statistic
  adf_diff_matrix[i, 2] <- result_diff[2]  # p-value
  adf_diff_matrix[i, 3] <- round(as.numeric(result_diff[4]), 2)  # critical values
}

knitr::kable(adf_diff_matrix, caption = "Table 4: ADF test on the second difference")
```



# Methodology
In this section, the model that this study use to capture the structural relationship between real economy variables and the stock price is introduced. 
The author use Structural Vector Autoregression (SVAR) model which comprises of 6 variables, _cashrate_, _realgdp_, _inflation_, _unemprate_, _stockprice_, and _govexp_. Subsequently, the model will be used to analyze the impulse responses.


The **SVAR model** can be shown as follows.

```{=tex}
\begin{align}
B_0y_t &= b_0 + \sum_{i=1}^{p}B_{i}y_{t-i}+u_t  \\
u_t|Y_{t-1}&\sim iid \mathcal{N} (0_N,I_N)
\end{align}
```

where :

$y_{t}$ is $N\times1$ vector of endogenous variables at time t

$B_0$ is $N\times N$ structural matrix which captures the contemporaneous relationships between variables

$u_t$ is $N\times1$ vector of conditionally on $Y_{t-1}$ orthogonal or independent structural shocks

<br>

Specifically, $y_{t}$ contains 6 variables as follows.
$$
y_t = \begin{bmatrix}
\text{cashrate} \\
\text{realgdp} \\
\text{CPI} \\
\text{unemprate} \\
\text{stockprice} \\
\text{govexp}
\end{bmatrix}
$$

The **reduced form** can be shown as follows.


```{=tex}
\begin{align}
y_t &= \mu_0 + \sum_{i=1}^{p}A_{i}y_{t-i}+\epsilon_t \\
\epsilon_t|Y_{t-1} &\sim iid \mathcal{N}(0_N,\Sigma)
\end{align}
```



where :

$A_i$ is $N\times N$ matrix of autoregressive slope parameters

$\mu_0$ is $N\times1$ vector of constant terms

$\epsilon_t$ is $N\times1$ vector of error terms - a multivariate white noise process

$\Sigma$ is $N\times N$ covariance matrix of the error term



# Estimation algorithm with simulated data
## The baseline model
### Estimation algorithm
 
The reduced form above can be rewrite in the matrix form as follows:

```{=tex}
\begin{align}
Y &= XA + E \\
E|X &\sim \mathcal{MN}_{T \times N}(0,\Sigma,I_T)
\end{align}
```



Bayes' rule is employed for deriving the posterior distribution

From Bayes' rule:

```{=tex}
\begin{align}
P(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X)P(A,\Sigma) \\
\underbrace{P(A,\Sigma|Y,X)}_{\text{Posterior distribution}} &\propto \underbrace{L(A,\Sigma|Y,X)}_{\text{Likelihood}} \underbrace{P(A|\Sigma)P(\Sigma)}_{\text{Prior distribution}}
\end{align}
```

<br>


#### The likelihood fuction
The derivation of the likelihood function can be shown in matrix-variate normal distribution as follows:

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) = (2\pi)^{-\frac{NT}{2}} det(\Sigma)^{-\frac{T}{2}}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\} \\
\end{align}
```


Then, the $\hat{A}$ and $\hat{\Sigma}$ corresponding to the maximum likelihood are

```{=tex}
\begin{align}
\hat{A} &= (X'X)^{-1}X'Y \\
\hat{\Sigma} &= \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})
\end{align}
```

  
The likelihood function can be represented as Normal inverse Wishart distribution 

```{=tex}
\begin{align}
L(A,\Sigma|Y,X) &\propto det(\Sigma)^{-\frac{T}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(Y-\hat{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
\end{align}
```

<br>

#### The prior distribution 


```{=tex}
\begin{align}
P(A,\Sigma) &= P(A|\Sigma) P(\Sigma) \\
A|\Sigma &\sim \mathcal{MN}_{K \times N} (\underline{A}, \Sigma , \underline{V}) \\
\Sigma &\sim \mathcal{IW}_{N}(\underline{S},\underline{\nu})
\end{align}
```

The prior which follows Normal inverse Wishart distribution:

```{=tex}
\begin{align}
p(A,\Sigma) &\propto det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\underline{S} \right] \right\}
\\
\end{align}
```



The Minnesota prior used as prior of the model, the parameters are follows:

```{=tex}
\begin{align}
\underline{A} &= [0_{N \times 1} \quad I_N \quad 0_{N \times (p-1)N}]' \\ 
Var[vec(A)] &= \Sigma \otimes  \underline{V} \\
\underline{V} &= \text{diag}([\kappa_2 \quad \kappa_1 (p^{-2} \otimes \imath_N)]) \\
\end{align}
```

where: 

$p$ = [1,2,...,p]

$\imath_N$ = [1,...,1]

$\kappa_1$ is overall shrinkage level for autoregressive slopes, the common value is $0.02^{2}$

$\kappa_2$ is overall shrinkage level for the constant term, the common value is $100$

This can be represented in code as follows:


```{r}
kappa.1 <- 0.02^2
kappa.2 <- 100 
```

```{r prior distribution}
#| echo: true
Prior.Dist  <- function (N,p,Sigma.hat,kappa.1,kappa.2) {

  K                 <- 1 + N*p 

  A.prior           <- matrix(0,K,N)
  A.prior[2:(N+1),] <- diag(N) 
  V.prior           <- diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior           <- diag(diag(Sigma.hat))
  nu.prior          <- N+1
  
  return (list(A.prior  = A.prior,
               V.prior  = V.prior,
               S.prior  = S.prior,
               nu.prior = nu.prior))
}
```


<br>

#### The posterior distribution 

From the Bayes' rule mentioned above, the posterior distribution as follows:


```{=tex}
\begin{align}
P(A,\Sigma|Y,X) \propto & L(A,\Sigma|Y,X)P(A,\Sigma) \\
P(A,\Sigma|Y,X) \propto 
& det(\Sigma)^{-\frac{T}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
&det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\underline{S} \right] \right\} \\
P(A,\Sigma|Y,X) \propto 
& det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}}
exp \left\{-\frac{1}{2} tr \left[
\Sigma^{-1}\left[(A-\hat{A})'X'X(A-\hat{A}) + (A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \\
+(Y-X\hat{A})'(Y-X\hat{A})+\underline{S})\right] \right] \right\}

\\

\end{align}
```


Which can represented in form of Normal inverse Wishart distribution

```{=tex}
\begin{align}
P(A,\Sigma|Y,X) &= P(A|Y,X,\Sigma) P(\Sigma|Y,X) \\
P(A|Y,X,\Sigma) &\sim \mathcal{MN}_{K \times N} (\overline{A}, \Sigma , \overline{V}) \\
P(\Sigma|Y,X) &\sim \mathcal{IW}_{N}(\overline{S},\overline{\nu})
\end{align}
```


<br>
And the posterior parameters are structured as follows.

```{=tex}
\begin{align}

\overline{V} &= (X'X+ \underline{V}^{-1})^{-1} \\ 
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1} \underline{A})\\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \underline{S}+Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - 
\overline{A}'\overline{V}^{-1}\overline{A} \\

\end{align}
```



This can be represented in code as follows:


```{r Normal-Inverse wishart posterior}
#| echo: true
Post.Dist <- function (X,Y,prior.parameters) {
  
  A.prior     <- prior.parameters$A.prior
  V.prior     <- prior.parameters$V.prior
  S.prior     <- prior.parameters$S.prior
  nu.prior    <- prior.parameters$nu.prior
  
  V.bar.inv   <- t(X)%*%X + diag(1/diag(V.prior))
  V.bar       <- solve(V.bar.inv)
  A.bar       <- V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
  nu.bar      <- nrow(Y) + nu.prior
  S.bar       <- S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
  S.bar.inv   <- solve(S.bar)
  
  return (list(V.bar      = V.bar,
               A.bar      = A.bar,
               nu.bar     = nu.bar,
               S.bar      = S.bar,
               S.bar.inv  = S.bar.inv,
               V.bar.inv  = V.bar.inv))
}
```



The draws from the posterior distribution can be represented in code as follows:


```{r draw posterior}
#| echo: true
Post.Draw <- function (N,S,p,posterior.parameters){
  
  K = 1+N*p
  
  A.bar               <- posterior.parameters$A.bar
  V.bar               <- posterior.parameters$V.bar
  S.bar               <- posterior.parameters$S.bar
  nu.bar              <- posterior.parameters$nu.bar
  S.bar.inv           <- posterior.parameters$S.bar.inv
  
  Sigma.posterior     <- rWishart(S, df=nu.bar, Sigma=S.bar.inv)  
  Sigma.posterior     <- apply(Sigma.posterior,3,solve)            
  Sigma.posterior     <- array(Sigma.posterior,c(N,N,S))   
  A.posterior         <- array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) 
  B0.tilde            <- array(NA,c(N,N,S))
  L                   <- t(chol(V.bar)) 
  Bplus.tilde         <- array(NA,c(N,K,S))
  
  for (s in 1:S){
    cholSigma.s       <- chol(Sigma.posterior[,,s])
    B0.tilde[,,s]     <- solve(t(cholSigma.s)) 
    A.posterior[,,s]  <- A.bar + L%*%A.posterior[,,s]%*%cholSigma.s 
    Bplus.tilde[,,s]  <- B0.tilde[,,s]%*%t(A.posterior[,,s])
  }

  return(list(A.posterior     = A.posterior,
              B0.tilde        = B0.tilde,
              Bplus.tilde     = Bplus.tilde,
              Sigma.posterior = Sigma.posterior)
         )
}
```


<br>

### Data simulation

The author generates 1,000 observations from bi-variate Gaussian random walk process to prove that the algorithm works. 


```{r data simulation}
set.seed(2024)
RW1  <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)
RW2  <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)

RW   <- cbind(RW1,RW2)
Y    <- RW[2:nrow(RW),]
```

```{r plot random walk}
plot.ts(Y,main = "1,000 simulations of bi-variate Gaussian random walk", xlab = "Observations", col = "grey27", lwd = 2, frame.plot = FALSE)
```

<div style="text-align: center;">  
##### Figure 5: The simulation data (1,000 observations) 
</div>

The target model for estimation is a Sign-Restricted Structural Vector Autoregression (SVAR).


```{=tex}
\begin{align}
B_0y_t &= b_0 + \sum_{i=1}^{p}B_{1}y_{t-1}+u_t  \\
u_t|Y_{t-1}&\sim iid \mathcal{N} (0_N,I_N)
\end{align}
```

```{r Set up variables}
#| echo: false
#| message: false
#| warning: false
X            <- matrix(1,nrow(Y),1)
X            <- cbind(X,RW[2: nrow(RW)-1,])
N            <- 2
p            <- 1
K            <- 1+p*N
S            <- 9000
```

```{r MLE}
#| echo: false
#| message: false
#| warning: false
A.hat        <- solve(t(X)%*%X)%*%t(X)%*%Y                
Sigma.hat    <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y) 
```


The author randomly draws matrices $A$ and $\Sigma$ from the posterior distribution,  which is specified as a Normal Inverse Wishart distribution as described earlier. Following this, the sign restriction is applied to the main diagonal of matrix $B_{0}$, ensuring positive signs.




```{r Restriction on B0}
#| echo: true
ImposeSignRestriction <- function (restrictions,N,p,posterior.draws){
  
  A.posterior         <- posterior.draws$A.posterior
  Sigma.posterior     <- posterior.draws$Sigma.posterior
  B0.tilde            <- posterior.draws$B0.tilde
  Bplus.tilde         <- posterior.draws$Bplus.tilde
  S                   <- dim(A.posterior)[3]
  B0.store            <- array(NA,c(N,N,S))
  Bplus.store         <- array(NA,c(N,K,S))
  
  i.vec = c()

  for (s in 1:S){
    B0.tilde1         <- B0.tilde[,,s]
    Bplus.tilde1      <- Bplus.tilde[,,s]
    
    i=1
    sign.restrictions.do.not.hold = TRUE 
    
    while (sign.restrictions.do.not.hold){
    X                 <- matrix(rnorm(N*N),N,N)         
    QR                <- qr(X, tol = 1e-10)
    Q                 <- qr.Q(QR,complete=TRUE)
    R                 <- qr.R(QR,complete=TRUE)
    Q                 <- t(Q %*% diag(sign(diag(R))))
    B0                <- Q%*%B0.tilde1                   
    Bplus             <- Q%*%Bplus.tilde1                  
    B0.inv            <- solve(B0)      
    check             <- all(diag(B0)>0)
    
    if (check){sign.restrictions.do.not.hold=FALSE}
    i=i+1 
    }
    i.vec             <- c(i.vec,i) 
    B0.store[,,s]     <- B0
    Bplus.store[,,s]  <- Bplus
  }
  
  return (list(B0.store    = B0.store,
               Bplus.store = Bplus.store,
               i           = i.vec))
}
```

```{r Estimate the baseline model}
prior.parameters      <- Prior.Dist(N,p,Sigma.hat,kappa.1,kappa.2)
posterior.parameters  <- Post.Dist(X,Y,prior.parameters)
posterior.draws       <- Post.Draw(N,S,p,posterior.parameters)
B.draws.restricted    <- ImposeSignRestriction(restrictions,N,p,posterior.draws)
B0.store              <- B.draws.restricted$B0.store
Bplus.store           <- B.draws.restricted$Bplus.store
B0.mean               <- apply(B0.store,1:2,mean)
Bplus.mean            <- apply(Bplus.store,1:2,mean)
A.post                <- posterior.draws$A.posterior
A.post.mean           <- apply(A.post,1:2,mean)
Sigma.post            <- posterior.draws$Sigma.posterior
Sigma.post.mean       <- apply(Sigma.post,1:2,mean)
```



Table 5 shows that the mean values of matrix $A$ at $A_{11}$ and $A_{22}$ are close to 1, while the constant term in the first row is nearly zero. Additionally, the variance-covariance matrix in Table 6 is approximately an identity matrix. These findings are consistent with our bi-variate Gaussian random walk process.



```{r A mean baseline}
A_df           <- as.data.frame(A.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "Cons", "Lag1", "Lag2")
knitr::kable(A_df, caption = "Table 5: Mean of the matrix A (The baseline model)")
```

```{r Sigma mean baseline}
A_df           <- as.data.frame(Sigma.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "R1", "R2")
knitr::kable(A_df, caption = "Table 6: Mean of the variance-covariance matrix (The baseline model)")
```


<br>

Table 7 displays that the mean values of the main diagonal elements in matrix $B_{0}$ are positive, providing evidence for the validity of the imposed restriction.


```{r B0 mean baseline}
B0_df           <- as.data.frame(B0.mean)
colnames(B0_df) <- c("C1", "C2")
rownames(B0_df) <- c( "R1", "R2")
knitr::kable(B0_df, caption = "Table 7: Mean of the matrix B0 (The baseline model)")
```

In Table 8 which displays the mean of matrix $B_{+}$, the values in the first column, representing the constant term in the equation, are close to zero.


```{r Bplus mean baseline}
Bplus_df           <- as.data.frame(Bplus.mean)
colnames(Bplus_df) <- c("C1","C2", "C3")
rownames(Bplus_df) <- c( "R1", "R2")
knitr::kable(Bplus_df, caption = "Table 8: Mean of the matrix B+ (The baseline model)")
```


## The extension model
### Estimation algorithm
Suppose the author aims to enhance the flexibility of the prior distribution for gamma ($\gamma$) by modeling it in the following manner.


```{=tex}
\begin{align}
\color{Grey}P(A,\Sigma\color{Black}|\gamma) &= \color{Grey}P(A|\Sigma) P(\Sigma\color{Black}|\gamma) \\
\color{Grey}A|\Sigma &\sim \color{Grey}\mathcal{MN}_{K \times N} \color{Grey}(\underline{A}, \Sigma , \underline{V}) \\
\color{Black}\Sigma|\gamma &\sim \mathcal{IW}_{N}(\gamma I_{N},\underline{\nu}) \\
\gamma &\sim \mathcal{IG2} (\underline{S},\underline{\nu})
\end{align}
```


Thus, the the posterior distribution as follows: 


```{=tex}
\begin{align}
P(A,\Sigma|Y,X,\gamma) \propto & L(A,\Sigma|Y,X,\gamma)P(A,\Sigma) \\
P(A,\Sigma|Y,X,\gamma) \propto & L(A,\Sigma|Y,X,\gamma)P(A|\Sigma) P(\Sigma|\gamma) \\
P(A,\Sigma|Y,X,\gamma) \propto 
& det(\Sigma)^{-\frac{T}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
&det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\gamma I_{N} \right] \right\} \\
P(A,\Sigma|Y,X,\gamma) \propto 
& det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}}
exp \left\{-\frac{1}{2} tr \left[
\Sigma^{-1}\left[(A-\hat{A})'X'X(A-\hat{A}) + (A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \\
+(Y-X\hat{A})'(Y-X\hat{A})+\gamma I_{N}\right] \right] \right\} \\

\end{align}
```


The distribution represented in the form of Normal Inverse Wishart is characterized by the following parameters:

```{=tex}
\begin{align}

\overline{V} &= (X'X+ \underline{V}^{-1})^{-1} \\ 
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1} \underline{A})\\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \gamma I_{N}+Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - 
\overline{A}'\overline{V}^{-1}\overline{A} \\

\end{align}
```


<br>

The posterior distribution of $\gamma$ is also needed as it serves as a crucial component for the sampling procedure.


```{=tex}
\begin{align}
P(\gamma|A,\Sigma,Y,X) \propto & L(Y|\Sigma,X,A)P(\gamma)P(A,\Sigma)P(\Sigma|\gamma) \\
\propto & P(\gamma)P(\Sigma|\gamma) \\

= & \gamma^{-\frac{\underline{\nu}+2}{2}}\:exp\left\{{-\frac{1}{2}\frac{\underline{S}}{\lambda}} \right\} \:det(\gamma I_{N})^{\frac{\underline{\nu}}{2}}\:det(\Sigma)^{-\frac{\underline{\nu}+N+1}{2}}\:exp\left\{{-\frac{1}{2}tr[\Sigma^{-1}\gamma I_{N}]} \right\} \\

= &\gamma^{-\frac{\underline{\nu}+2}{2}}\:exp\left\{{-\frac{1}{2}\frac{\underline{S}}{\gamma}} \right\}\:\gamma^{-\frac{N\underline{\nu}}{2}}\:det(\Sigma)^{\frac{\underline{\nu}+N+1}{2}}\:exp\left\{{-\frac{1}{2}\gamma \:tr[\Sigma^{-1}]} \right\} \\

\propto &\gamma^{\frac{N\underline{\nu}-\underline{\nu}}{2}-1} \: 
exp\left\{{-\frac{1}{2}}[\gamma\:tr[\Sigma^{-1}]+{\frac{\underline{S}}{\gamma}}]\right\} \\

\end{align}
```


The following distribution conforms to the Generalized Inverse Gaussian (GIG) distribution, with the parameters as follows:


```{=tex}
\begin{align}
\lambda = {\frac{N\underline{\nu}-\underline{\nu}}{2}} \\

\\ \chi = \underline{S}\\

\\ \psi = tr[\Sigma^{-1}]
\end{align}
```


The Gibbs Sampler is used to get the posterior draws of the extended model

For $S_{1} = 1,000$ and $S_{2} = 9,000$

Initialize $\gamma$ at $\gamma^{(0)}$

At each iteration:

1) Draw $\Sigma^{(s)} \sim P(\Sigma|Y,X,\gamma^{(s-1)})$
2) Draw $A^{(s)}\sim P(A|\Sigma^{(s)},Y,X)$ 
3) Draw $\gamma^{(s)} \sim P(\gamma|Y,X,A^{(s)},\Sigma^{(s)})$

Repeat steps 1 to 3 for $S_{1}$ + $S_{2}$ iterations

Discard the first $S_{1}$ iterations to allow the algorithm to converge to the stationary state.

The author then utilizes the random draws from $\Sigma^{(s)}, A^{(s)}, \gamma^{(s)}$ to obtain the posterior draw and create the following $B_{0}$ and $B_{+}$ matrices.

This can be represented in code as follows:


```{r}
S1 = 1000
S2 = S-S1
```

```{r Function for extension model}
#| echo: true
Gibbsampler.Extension <- function(Y, prior.parameters, posterior.parameters, S1, S2, N, K) {
  A.prior             <- prior.parameters$A.prior
  V.prior             <- prior.parameters$V.prior
  nu.prior            <- prior.parameters$nu.prior
  S.prior             <- prior.parameters$S.prior
  A.bar               <- posterior.parameters$A.bar
  V.bar               <- posterior.parameters$V.bar
  V.bar.inv           <- posterior.parameters$V.bar.inv
  nu.bar              <- posterior.parameters$nu.bar
  
  # Initialize variables
  S.bar               <- array(0, dim = c(N, N, (S1+S2)))
  S.bar.inv           <- array(0, dim = c(N, N, (S1+S2)))
  gamma.store         <- numeric(S1+S2)
  gamma.store[1]      <- 1
  A.posterior         <- array(0, dim = c(K, N, (S1+S2)))
  B0.tilde            <- array(0, dim = c(N, N, (S1+S2)))
  Bplus.tilde         <- array(0, dim = c(N, K, (S1+S2)))
  Sigma.store         <- array(0, dim = c(N, N, (S1+S2)))
  i.vec = c()
  
  for (i in 1:(S1+S2)) {
    # Calculate S.bar
    S.bar[,,i]        <- gamma.store[i] * diag(N) + t(Y) %*% Y + t(A.prior) %*% diag(1/diag(V.prior)) %*% A.prior - t(A.bar) %*% V.bar.inv %*% A.bar
    S.bar.inv[,,i]    <- solve(S.bar[,,i])
    
    # Generate posterior samples using rWishart
    Sigma.posterior   <- rWishart(1, df = nu.bar, Sigma = S.bar.inv[,,i])
    Sigma.posterior   <- apply(Sigma.posterior, 3, solve)
    Sigma.posterior   <- array(Sigma.posterior, c(N, N, 1))
    Sigma.store[,,i]  <- Sigma.posterior[,,1]
    cholSigma.s       <- chol(Sigma.posterior[,,1])
    L                 <- t(chol(V.bar)) 
    A.posterior[,,i]  <- A.bar + L %*% A.posterior[,,i] %*% cholSigma.s
    
    # Compute lambda, chi, and psi
    lambda            <- ((nu.prior * N) - nu.prior) / 2
    chi               <- S.prior
    psi               <- sum(diag(solve(Sigma.posterior[,,1])))
    
    # Generate gamma using rgig
    gamma.store[i]    <- GIGrvg::rgig(n = 1, lambda, chi, psi)
    
    # Update B0.tilde and Bplus.tilde
    B0.tilde[,,i]     <- solve(t(cholSigma.s))
    Bplus.tilde[,,i]  <- B0.tilde[,,i] %*% t(A.posterior[,,i])
  }

  return(list(A.posterior      = A.posterior[,,S1:(S1+S2)],
               gamma.store     = gamma.store[S1:(S1+S2)],
               B0.tilde        = B0.tilde[,,S1:(S1+S2)],
               Bplus.tilde     = Bplus.tilde[,,S1:(S1+S2)],
               Sigma.posterior = Sigma.store[,,S1:(S1+S2)],
               i               = i.vec))
}
```

```{r}
posterior.draws     <- Gibbsampler.Extension(Y, prior.parameters, posterior.parameters, S1, S2, N, K)
B.draws.restricted  <- ImposeSignRestriction(restrictions,N,p,posterior.draws)
B0.store            <- B.draws.restricted$B0.store
Bplus.store         <- B.draws.restricted$Bplus.store
gamma.store         <- posterior.draws$gamma.store
B0.mean             <- apply(B0.store,1:2,mean)
Bplus.mean          <- apply(Bplus.store,1:2,mean)
gamma.mean          <- round(mean(gamma.store), digits = 4)
A.post              <- posterior.draws$A.posterior
A.post.mean         <- apply(A.post,1:2,mean)
Sigma.post          <- posterior.draws$Sigma.posterior
Sigma.post.mean     <- apply(Sigma.post,1:2,mean)
```



### Data simulation

Table 9 shows that the mean values of matrix $A$ and the variance-covariance matrix in Table 10 exhibit behavior similar to that of the basic model.


```{r A mean extension}
A_df           <- as.data.frame(A.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "Cons", "Lag1", "Lag2")
knitr::kable(A_df, caption = "Table 9: Mean of the matrix A (The extension model)")
```

```{r Sigma mean extension}
A_df           <- as.data.frame(Sigma.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "R1", "R2")
knitr::kable(A_df, caption = "Table 10: Mean of the variance-covariance matrix (The extension model)")
```

<br>

Table 11 indicates that the mean value of the main diagonal of $B_{0}$ remains positive, exhibiting a similar behavior to that of the basic model.


```{r B0 mean for extension}
B0_df           <- as.data.frame(B0.mean)
colnames(B0_df) <- c("C1", "C2")
rownames(B0_df) <- c( "R1", "R2")
knitr::kable(B0_df, caption = "Table 11: Mean of the matrix B0 (The extension model)")
```


In Table 12 which displays the mean of matrix $B_{+}$, the values in the first column, representing the constant term in the equation, are close to zero and exhibit a behavior similar to that of the basic model.


```{r Bplus mean for extension}
Bplus_df           <- as.data.frame(Bplus.mean)
colnames(Bplus_df) <- c("C1","C2", "C3")
rownames(Bplus_df) <- c( "R1", "R2")
knitr::kable(Bplus_df, caption = "Table 12: Mean of the matrix B+ (The extension model)")
```


The mean of $\gamma$ is equal `r gamma.mean` and the trace plot and histogram shown in Figure 6.


```{r}
par(mfrow = c(1, 2))
plot(gamma.store,type='l',col="slategray4",ylab=expression(gamma),xlab="no. of iterations",main=expression("Plot of " * gamma), lwd = 0.1)
abline(h=gamma.mean,lty=3)
hist(gamma.store, breaks=500, border="steelblue",xlab=expression(gamma), main=expression("Histogram of " * gamma))
```

<div style="text-align: center;">  
##### Figure 6: The plot and histogram of gamma (The extension model)
</div>


<br>


## Stochastic Volatility Heteroskedasticity
### Estimation algorithm

Next, the author will estimate the model with stochastic volatility. This approach allows for the capture of time-varying volatility in the data. The model will be applied to both the baseline model and the extension model.

The reduced form above can be rewrite in the matrix form as follows:


```{=tex}
\begin{align}
Y &= XA + E \\
E|X &\sim \mathcal{MN}_{T \times N}(0,\Sigma,diag(\sigma^{2}))
\end{align}
```


Where $\sigma^{2} = (exp\{h_{1}\},exp\{h_{2}\},\cdots, exp\{h_{T}\})$ which is a vector of conditional heteroskedasticity variables and $h_{t}$ follows a stochastic volatility process. 

Thus, the likelihood can be rewrite as 


```{=tex}
\begin{align}
L(A,\Sigma|Y,X,\sigma^{2}) \propto det(diag(\sigma^{2}))^{-\frac{N}{2}} det(\Sigma)^{-\frac{T}{2}}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(diag(\sigma^{2}))^{-1}(Y-XA) \right] \right\} \\
\end{align}
```


#### **The baseline model**

The full conditional posterior for the baseline model is in the form of a Normal Inverse Wishart distribution, with posterior parameters structured as follows.


```{=tex}
\begin{align}

\overline{V} &= (X'(diag(\sigma^{2})^{-1}X+ \underline{V}^{-1})^{-1} \\ 
\overline{A} &= \overline{V}(X'(diag(\sigma^{2})^{-1}Y+\underline{V}^{-1} \underline{A})\\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \underline{S}+Y'(diag(\sigma^{2})^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - 
\overline{A}'\overline{V}^{-1}\overline{A} \\

\end{align}
```



The Gibbs Sampler is used to get the posterior draws of the stochastic volatility model.

For $S_{1} = 1,000$ and $S_{2} = 9,000$

Initialize $\sigma^{2}$ at $\sigma^{2^{(0)}}$

At each iteration s:

1) Draw $(A,\Sigma)^{(s)} \sim P(A,\Sigma|Y,X,\sigma^{2^{(s-1)}})$
2) Draw $\sigma^{2^{(s)}} \sim P(\sigma^{2}|Y,X,A^{(s)},\Sigma^{(s)})$ 

Repeat step 1 and 2 for $(S_{1} + S_{2})$ times, then discard the first $S_{1}$ iterations to allow the algorithm to converge to the equilibrium state.

The function for drawing $\sigma^{2^{(s)}} \sim P(\sigma^{2}|Y,X,A^{(s)},\Sigma^{(s)})$ is as follows.



```{r Function for drawing sigma square}
#| echo: true
SVcommon.Gibbs.iteration = function(aux, priors){
  # A single iteration of the Gibbs sampler for the SV component
  #
  # aux is a list containing:
  #   Y - a TxN matrix
  #   X - a TxK matrix
  #   H - a Tx1 matrix
  #   h0 - a scalar
  #   sigma.v2 - a scalar
  #   s - a Tx1 matrix
  #   A - a KxN matrix
  #   Sigma - an NxN matrix
  #   sigma2 - a Tx1 matrix
  #
  # priors is a list containing:
  #   h0.v - a positive scalar
  #   h0.m - a scalar
  #   sigmav.s - a positive scalar
  #   sigmav.nu - a positive scalar
  #   HH - a TxT matrix
  
  T             <- dim(aux$Y)[1]
  N             <- dim(aux$Y)[2]
  alpha.st      <- c(1.92677,1.34744,0.73504,0.02266,0-0.85173,-1.97278,-3.46788,-5.55246,-8.68384,-14.65000)
  sigma.st      <- c(0.11265,0.17788,0.26768,0.40611,0.62699,0.98583,1.57469,2.54498,4.16591,7.33342)
  pi.st         <- c(0.00609,0.04775,0.13057,0.20674,0.22715,0.18842,0.12047,0.05591,0.01575,0.00115)
  
  Lambda        <- solve(chol(aux$Sigma))
  Z             <- rowSums((aux$Y - aux$X %*% aux$A ) %*% Lambda) / sqrt(N)
  Y.tilde       <- as.vector(log((Z + 0.0000001)^2))
  Ytilde.alpha  <- as.matrix(Y.tilde - alpha.st[as.vector(aux$s)])
  
  # sampling initial condition
  ############################################################
  V.h0.bar      <- 1/((1 / priors$h0.v) + (1 / aux$sigma.v2))
  m.h0.bar      <- V.h0.bar*((priors$h0.m / priors$h0.v) + (aux$H[1] / aux$sigma.v2))
  h0.draw       <- rnorm(1, mean = m.h0.bar, sd = sqrt(V.h0.bar))
  aux$h0        <- h0.draw
  
  # sampling sigma.v2
  ############################################################
  sigma.v2.s    <- priors$sigmav.s + sum(c(aux$H[1] - aux$h0, diff(aux$H))^2)
  sigma.v2.draw <- sigma.v2.s / rchisq(1, priors$sigmav.nu + T)
  aux$sigma.v2  <- sigma.v2.draw
  
  # sampling auxiliary states
  ############################################################
  Pr.tmp        <- simplify2array(lapply(1:10,function(x){
    dnorm(Y.tilde, mean = as.vector(aux$H + alpha.st[x]), sd = sqrt(sigma.st[x]), log = TRUE) + log(pi.st[x])
  }))
  Pr            <- t(apply(Pr.tmp, 1, function(x){exp(x - max(x)) / sum(exp(x - max(x)))}))
  s.cum         <- t(apply(Pr, 1, cumsum))
  r             <- matrix(rep(runif(T), 10), ncol = 10)
  ss            <- apply(s.cum < r, 1, sum) + 1
  aux$s         <- as.matrix(ss)
  
  
  # sampling log-volatilities using functions for tridiagonal precision matrix
  ############################################################
  Sigma.s.inv   <- diag(1 / sigma.st[as.vector(aux$s)])
  D.inv         <- Sigma.s.inv + (1 / aux$sigma.v2) * priors$HH
  b             <- as.matrix(Ytilde.alpha / sigma.st[as.vector(aux$s)] + (aux$h0/aux$sigma.v2)*diag(T)[,1])
  lead.diag     <- diag(D.inv)
  sub.diag      <- mgcv::sdiag(D.inv, -1)
  D.chol        <- mgcv::trichol(ld = lead.diag, sd = sub.diag)
  D.L           <- diag(D.chol$ld)
  mgcv::sdiag(D.L,-1) = D.chol$sd
  x             <- as.matrix(rnorm(T))
  a             <- forwardsolve(D.L, b)
  draw          <- backsolve(t(D.L), a + x)
  aux$H         <- as.matrix(draw)
  aux$sigma2    <- as.matrix(exp(draw))

  return(aux)
}
```


And the function for drawing $(A,\Sigma)^{(s)} \sim P(A,\Sigma|Y,X,\sigma^{2^{(s-1)}})$ is as follows.



```{r Function for baseline and stochastic model}
#| echo: true
Post.draws.stochastic <- function (Y,X,prior.parameters.stochastic,S1,S2) {
  
  N                     <- ncol(Y)
  K                     <- ncol(X)
  T                     <- nrow(Y)
  
  A.prior               <- prior.parameters.stochastic$A.prior
  V.prior               <- prior.parameters.stochastic$V.prior
  S.prior               <- prior.parameters.stochastic$S.prior
  nu.prior              <- prior.parameters.stochastic$nu.prior
  
  A.posterior           <- array(NA,c(K,N,(S1+S2)))
  Sigma.posterior       <- array(NA,c(N,N,(S1+S2))) 
  H.posterior           <- array(NA,c(nrow(Y),(S1+S2+1)))  
  B0.tilde              <- array(NA,c(N,N,(S1+S2)))
  Bplus.tilde           <- array(NA,c(N,K,(S1+S2)))
  
  H.posterior[,1]       <- matrix(1,T,1)
  HH                    <- 2*diag(T)
  mgcv::sdiag(HH,-1)    <- -1
  mgcv::sdiag(HH,1)     <- -1
  
  priors = list(HH       = HH,
                h0.m     = 0,
                h0.v     = 1,
                sigmav.s = 1,
                sigmav.nu= 1
              )
  
  for (s in 1:(S1+S2)){
    V.bar.inv            <- t(X)%*%diag(1/H.posterior[,s])%*%X + diag(1/diag(V.prior))
    V.bar                <- solve(V.bar.inv)
    A.bar                <- V.bar%*%(t(X)%*%diag(1/H.posterior[,s])%*%Y + diag(1/diag(V.prior))%*%A.prior)
    nu.bar               <- nrow(Y) + nu.prior
    S.bar                <- S.prior + t(Y)%*%diag(1/H.posterior[,s])%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    S.bar.inv            <- solve(S.bar)
    Sigma.inv.posterior  <- rWishart(1, df = nu.bar, Sigma = S.bar.inv)
    Sigma.posterior.draw <- apply(Sigma.inv.posterior, 3, solve)
    Sigma.posterior[,,s] <- Sigma.posterior.draw
    cholSigma.s          <- chol(Sigma.posterior[,,s])
    A.posterior[,,s]     <- matrix(MASS::mvrnorm(1, as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N) 
    L                    <- t(chol(V.bar))
    
    B0.tilde[,,s]        <- solve(t(cholSigma.s))
    Bplus.tilde[,,s]     <- B0.tilde[,,s]%*%t(A.posterior[,,s])
  
    if (s == 1){
        aux = list( 
              Y               = Y,
              X               = X,
              H               = matrix(1,T,1),
              h0              = 0,
              sigma.v2        = 1,
              s               = matrix(1,T,1),
              Sigma           = Sigma.posterior[,,s],
              A               = A.posterior[,,s],
              sigma2          = matrix(1,T,1)
              )
    }else{
      aux = list(
                Y             = Y,
                X             = X,
                H             = tmp$H,
                h0            = tmp$h0,
                sigma.v2      = tmp$sigma.v2,
                s             = tmp$s,
                Sigma         = Sigma.posterior[,,s],
                A             = A.posterior[,,s],
                sigma2        = tmp$sigma2
              )
    }
      
    tmp                       <- SVcommon.Gibbs.iteration(aux,priors)
    H.posterior[,s+1]         <- as.matrix(tmp$sigma2)
  }
    
  return(list(Sigma.posterior = Sigma.posterior[,,(S1+1):(S1+S2)],
              A.posterior     = A.posterior[,,(S1+1):(S1+S2)],
              Bplus.tilde     = Bplus.tilde[,,(S1+1):(S1+S2)],
              B0.tilde        = B0.tilde[,,(S1+1):(S1+S2)],
              H.sv            = H.posterior[,(S1+2):(S1+S2+1)]))
}

```


<br>

#### **The extension model**

The posterior parameters from the full conditional posterior for the extension model structured as follows.


```{=tex}
\begin{align}

\overline{V} &= (X'(diag(\sigma^{2})^{-1}X+ \underline{V}^{-1})^{-1} \\ 
\overline{A} &= \overline{V}(X'(diag(\sigma^{2})^{-1}Y+\underline{V}^{-1} \underline{A})\\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \gamma I_{N} + Y'(diag(\sigma^{2})^{-1}Y + \underline{A}'\underline{V}^{-1}\underline{A} - 
\overline{A}'\overline{V}^{-1}\overline{A} \\

\end{align}
```


The Gibbs Sampler is used to get the posterior draws of the stochastic volatility model.

For $S_{1} = 1,000$ and $S_{2} = 9,000$

Initialize $\sigma^{2}$ at $\sigma^{2^{(0)}}$ and $\gamma$ at $\gamma^{(0)}$

At each iteration s:

1) Draw $\Sigma^{(s)} \sim P(\Sigma|Y,X,\gamma^{(s-1)},\sigma^{2^{(s-1)}})$
2) Draw $A^{(s)}\sim P(A|\Sigma^{(s)},Y,X,\sigma^{2^{(s-1)}})$ 
3) Draw $\gamma^{(s)} \sim P(\gamma|Y,X,A^{(s)},\Sigma^{(s)},\sigma^{2^{(s-1)}})$
4) Draw $\sigma^{2^{(s)}} \sim P(\sigma^{2}|Y,X,A^{(s)},\Sigma^{(s)},\gamma^{(s)})$ 

Repeat step 1 to 4 for $(S_{1} + S_{2})$ times, then discard the first $S_{1}$ iterations to allow the algorithm to converge to the equilibrium state.

The function for drawing $\sigma^{2^{(s)}} \sim P(\sigma^{2}|Y,X,A^{(s)},\Sigma^{(s)},\gamma^{(s)})$ is the same as in the baseline model since $\gamma$ does not affect the distribution, while the function for drawing steps 1 to 3 is as follows.



```{r Function for extension and stochastic model}
#| echo: true
Post.draws.stochastic.extension <- function (Y,X,prior.parameters.stochastic,S1,S2) {
  
  N                     <- ncol(Y)
  K                     <- ncol(X)
  T                     <- nrow(Y)
  
  A.prior               <- prior.parameters.stochastic$A.prior
  V.prior               <- prior.parameters.stochastic$V.prior
  S.prior               <- prior.parameters.stochastic$S.prior
  nu.prior              <- prior.parameters.stochastic$nu.prior
  
  A.posterior           <- array(NA,c(K,N,(S1+S2)))
  Sigma.posterior       <- array(NA,c(N,N,(S1+S2))) 
  H.posterior           <- array(NA,c(nrow(Y),(S1+S2+1)))  
  B0.tilde              <- array(NA,c(N,N,(S1+S2)))
  Bplus.tilde           <- array(NA,c(N,K,(S1+S2)))
  gamma.store           <- numeric(S1+S2)
  
  H.posterior[,1]       <- matrix(1,T,1)
  HH                    <- 2*diag(T)
  mgcv::sdiag(HH,-1)    <- -1
  mgcv::sdiag(HH,1)     <- -1

  nu.bar         <- nrow(Y) + nu.prior
  
  priors = list(HH       = HH,
                h0.m     = 0,
                h0.v     = 1,
                sigmav.s = 1,
                sigmav.nu= 1
              )
  
  for (s in 1:(S1+S2)){
    V.bar.inv            <- t(X)%*%diag(1/H.posterior[,s])%*%X + diag(1/diag(V.prior))
    V.bar                <- solve(V.bar.inv)
    A.bar                <- V.bar%*%(t(X)%*%diag(1/H.posterior[,s])%*%Y + diag(1/diag(V.prior))%*%A.prior)
    S.bar                <- gamma.store[s] * diag(N) + t(Y)%*%diag(1/H.posterior[,s])%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
    
    Sigma.inv.posterior  <- rWishart(1, df = nu.bar, Sigma = solve(S.bar))
    Sigma.posterior.draw <- apply(Sigma.inv.posterior, 3, solve)
    Sigma.posterior[,,s] <- Sigma.posterior.draw

    cholSigma.s          <- chol(Sigma.posterior[,,s])
    A.posterior[,,s]     <- matrix(MASS::mvrnorm(1, as.vector(A.bar), Sigma.posterior[,,s]%x%V.bar),ncol=N) 
    L                    <- t(chol(V.bar))
    
    lambda               <- ((nu.prior * N) - nu.prior) / 2
    chi                  <- S.prior
    psi                  <- sum(diag(solve(Sigma.posterior[,,1])))
    gamma.store[s]       <- GIGrvg::rgig(n = 1, lambda, chi, psi)
  
    B0.tilde[,,s]        <- solve(t(chol(Sigma.posterior[,,s])))  
    Bplus.tilde[,,s]     <- B0.tilde[,,s]%*%t(A.posterior[,,s])
  
    if (s == 1){
        aux = list( 
              Y             = Y,
              X             = X,
              H             = matrix(1,T,1),
              h0            = 0,
              sigma.v2      = 1,
              s             = matrix(1,T,1),
              Sigma         = Sigma.posterior[,,s],
              A             = A.posterior[,,s],
              sigma2        = matrix(1,T,1),
              gamma         = 1
              )
    }else{ 
      aux = list(
                Y           = Y,
                X           = X,
                H           = tmp$H,
                h0          = tmp$h0,
                sigma.v2    = tmp$sigma.v2,
                s           = tmp$s,
                Sigma       = Sigma.posterior[,,s],
                A           = A.posterior[,,s],
                sigma2      = tmp$sigma2,
                gamma       = gamma.store[s]
              )
    }
      
    tmp                     <- SVcommon.Gibbs.iteration(aux,priors)
    H.posterior[,s+1]       <- as.matrix(tmp$sigma2)
  }
    
  return(list(Sigma.posterior = Sigma.posterior[,,(S1+1):(S1+S2)],
              A.posterior     = A.posterior[,,(S1+1):(S1+S2)],
              Bplus.tilde     = Bplus.tilde[,,(S1+1):(S1+S2)],
              B0.tilde        = B0.tilde[,,(S1+1):(S1+S2)],
              H.sv            = H.posterior[,(S1+2):(S1+S2+1)],
              gamma.store     = gamma.store[(S1+1):(S1+S2)]))
}

```


### Data simulation

#### **The baseline model**


```{r Estimate baseline and stochastic model}
prior.parameters.stochastic <- Prior.Dist(N,p,Sigma.hat,kappa.1,kappa.2)
posterior.draws             <- Post.draws.stochastic(Y,X,prior.parameters.stochastic,S1,S2)
B.draws.restricted          <- ImposeSignRestriction(restrictions,N,p,posterior.draws)
B0.store                    <- B.draws.restricted$B0.store
Bplus.store                 <- B.draws.restricted$Bplus.store
B0.mean                     <- apply(B0.store,1:2,mean)
Bplus.mean                  <- apply(Bplus.store,1:2,mean)
A.post                      <- posterior.draws$A.posterior
A.post.mean                 <- apply(A.post,1:2,mean)
Sigma.post                  <- posterior.draws$Sigma.posterior
Sigma.post.mean             <- apply(Sigma.post,1:2,mean)
```


Table 13 demonstrates that the mean values of matrix $A$ remain consistent with those in previous models. However, in Table 14, the variance-covariance matrix has main diagonal values that are nearly zero, deviating significantly from an identity matrix. This contrasts with the model that does not incorporate stochastic volatility.



```{r A mean baseline SV}
A_df           <- as.data.frame(A.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "Cons", "Lag1", "Lag2")
knitr::kable(A_df, caption = "Table 13: Mean of the matrix A (The baseline with stochastic volatility model)")
```

```{r Sigma mean baseline SV}
A_df           <- as.data.frame(Sigma.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "R1", "R2")
knitr::kable(A_df, caption = "Table 14: Mean of the variance-covariance matrix (The baseline with stochastic volatility model)")
```


The trace plot in Figure 7 confirms that the value of $A_{21}$ and $A_{32}$ oscillate around 1, while $\Sigma_{11}$ and $\Sigma_{22}$ oscillate around 0.002.


```{r}
par(mfrow = c(2, 2))
plot(A.post[2,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(A[21]), lwd = 0.05)
plot(A.post[3,2,],type='l',col="slategray4", ylab="",xlab="", main=expression(A[32]), lwd = 0.05)
plot(Sigma.post[1,1,],type='l',col="slategray3", ylab="",xlab="" , main=expression(Sigma[11]), lwd = 0.05)
plot(Sigma.post[2,2,],type='l',col="slategray3" ,ylab="" ,xlab="" ,main=expression(Sigma[22]), lwd = 0.05)
```

<div style="text-align: center;">  
##### Figure 7: The trace plot of A and Sigma (The baseline with stochastic volatility model)
</div>

<br>

Table 15 indicates that the mean value of the main diagonal of $B_{0}$ increase to around 14-15 which is significantly higher to the previous two models.


```{r B0 mean for baseline and stochastic model}
B0_df           <- as.data.frame(B0.mean)
colnames(B0_df) <- c("C1", "C2")
rownames(B0_df) <- c( "R1", "R2")
knitr::kable(B0_df, caption = "Table 15: Mean of the matrix B0 (The baseline with stochastic volatility model)")
```



In Table 16 which indicate the mean of the matrix $B_{+}$, the values in the first column, representing the constant term in the equation, are noticeably different from zero compared to the previous models.


```{r Bplus mean for baseline and stochastic model}
Bplus_df           <- as.data.frame(Bplus.mean)
colnames(Bplus_df) <- c("C1","C2", "C3")
rownames(Bplus_df) <- c( "R1", "R2")
knitr::kable(Bplus_df, caption = "Table 16: Mean of the matrix B+ (The baseline with stochastic volatility model)")
```




The trace plot and histogram in Figure 8 show that the values of $B_{11}$ and $B_{22}$ are widely spread from 0 to around 25, but all of them are positive, consistent with the imposed restriction.


```{r}
par(mfrow = c(2, 2))
plot(B0.store[1,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[11]), lwd = 0.05)
hist(B0.store[1,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[11]))
plot(B0.store[2,2,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[22]), lwd = 0.05)
hist(B0.store[2,2,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[22]))
```

<div style="text-align: center;">  
##### Figure 8: The trace plot and histogram of B0 matrix (The baseline with stochastic volatility model)
</div>

<br>


The posterior draw mean of $\sigma^{2}$ for each of period display in Figure 9. The $\sigma^{2}$ moves around 2000.


```{r}
sigma2.mean <- apply(posterior.draws$H.sv,1,mean) 
plot.ts(sigma2.mean,main=expression(paste("Posterior Mean of ",sigma^2)),
        ylab="",type='l',lwd=2,col="tomato4",xlab="T")
abline(h=mean(sigma2.mean),lty=2)
```


<div style="text-align: center;">  
##### Figure 9: The posterior mean of the stochastic volatility 
</div>

<br>

Table 17 shows that the mean values of matrix $A$ for lag 1 and lag 2 are equal 1, and the constant value is close to zero. Moreover, in Table 18, the variance-covariance matrix is very closely to zero. 

#### **The extension model**


```{r Estimate extension and stochastic model}

prior.parameters.stochastic <- Prior.Dist(N,p,Sigma.hat,kappa.1,kappa.2)
posterior.draws    <- Post.draws.stochastic.extension(Y,X,prior.parameters.stochastic,S1,S2)
B.draws.restricted <- ImposeSignRestriction(restrictions,N,p,posterior.draws)
B0.store           <- B.draws.restricted$B0.store
Bplus.store        <- B.draws.restricted$Bplus.store
B0.mean            <- apply(B0.store,1:2,mean)
Bplus.mean         <- apply(Bplus.store,1:2,mean)
gamma.store        <- posterior.draws$gamma.store
gamma.mean         <- round(mean(gamma.store), digits = 4)
A.post             <- posterior.draws$A.posterior
A.post.mean        <- apply(A.post,1:2,mean)
Sigma.post         <- posterior.draws$Sigma.posterior
Sigma.post.mean    <- apply(Sigma.post,1:2,mean)
```

```{r A mean extension SV}
A_df           <- as.data.frame(A.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "Cons", "Lag1", "Lag2")
knitr::kable(A_df, caption = "Table 17: Mean of the matrix A (The extension with stochastic volatility model)")
```

```{r Sigma mean extension SV}
A_df           <- as.data.frame(Sigma.post.mean)
colnames(A_df) <- c("RW1", "RW2")
rownames(A_df) <- c( "R1", "R2")
knitr::kable(A_df, digits = 10, caption = "Table 18: Mean of the variance-covariance matrix (The extension with stochastic volatility model)")
```


The trace plot and histogram depicted in Figure 10 indicate that the values of $A_{21}$ and $A_{32}$ fluctuate around 1, whereas $\Sigma_{11}$ and $\Sigma_{22}$ are nearly zero.



```{r}
par(mfrow = c(2, 2))
plot(A.post[2,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(A[21]), lwd = 0.05)
plot(A.post[3,2,],type='l',col="slategray4", ylab="",xlab="", main=expression(A[32]), lwd = 0.05)
plot(Sigma.post[1,1,],type='l',col="slategray3", ylab="",xlab="" , main=expression(Sigma[11]), lwd = 0.05)
plot(Sigma.post[2,2,],type='l',col="slategray3" ,ylab="" ,xlab="" ,main=expression(Sigma[22]), lwd = 0.05)
```


<div style="text-align: center;">  
##### Figure 10: The trace plot of A and Sigma (The extension with stochastic volatility model)
</div>

<br>


Table 19 indicates that the mean value of the matrix $B_{0}$ is notably elevated, which diverges markedly from all previous models. And the mean value of the matrix $B_{+}$ display in Table 20.


```{r B0 mean for extension SV}
B0_df           <- as.data.frame(B0.mean)
colnames(B0_df) <- c("C1", "C2")
rownames(B0_df) <- c( "R1", "R2")
knitr::kable(B0_df, caption = "Table 19: Mean of the matrix B0 (The extension with stochastic volatility model)")
```

```{r Bplus mean for extension and stochastic model}
Bplus_df           <- as.data.frame(Bplus.mean)
colnames(Bplus_df) <- c("C1","C2", "C3")
rownames(Bplus_df) <- c( "R1", "R2")
knitr::kable(Bplus_df, caption = "Table 20: Mean of the matrix B+ (The extension with stochastic volatility model)")
```


The trace plot and histogram in Figure 11 reveal that the values of $B_{11}$ and $B_{22}$ span widely from zero to approximately million, confirming the high mean value reported in Table 19.



```{r}
par(mfrow = c(2, 2))
plot(B0.store[1,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[11]), lwd = 0.05)
hist(B0.store[1,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[11]))
plot(B0.store[2,2,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[22]), lwd = 0.05)
hist(B0.store[2,2,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[22]))
```

<div style="text-align: center;">  
##### Figure 11: The trace plot and histogram of B0 matrix (The extension with stochastic volatility model)
</div>

<br>

The mean value of $\gamma$ is equal `r gamma.mean`, which is close to that of the extension model without incorporating stochastic volatility heteroskedasticity. The plot and histogram shown in Figure 12.


```{r}
par(mfrow = c(1, 2))
plot(gamma.store,type='l',col="steelblue",ylab=expression(gamma),xlab="no. of iterations",main=expression("Plot of " * gamma), lwd = 0.1)
abline(h=gamma.mean,lty=3)
hist(gamma.store, breaks=500, border="steelblue",xlab=expression(gamma), main=expression("Histogram of " * gamma))
```

<div style="text-align: center;">  
##### Figure 12: The plot and histogram of gamma (The extension with stochastic volatility model)
</div>

<br>

The posterior draw mean of $\sigma^{2}$ for each of period is displayed in Figure 13. For certain periods, $\sigma^{2}$ exhibits a significant increase to a very large value.





```{r}
sigma2.mean <- apply(posterior.draws$H.sv,1,mean) 
plot.ts(sigma2.mean,main=expression(paste("Posterior Mean of ",sigma^2)),
        ylab="",type='l',lwd=2,col="tomato4",xlab="T")
abline(h=mean(sigma2.mean),lty=2)
```

<div style="text-align: center;">  
##### Figure 13: The posterior mean of the stochastic volatility (The extension with stochastic volatility model) 
</div>


<br>


Once we prove that the algorithm and coding can work with the simulated random walk data, the author will apply the same procedure to the empirical data, which contains 6 variables as indicated in the methodology section.

# Empirical estimation


```{r Set up variables_Empirical}
#| echo: false
#| message: false
#| warning: false

p            <- 5   # deal with quarterly data
Y            <- data[(1+p):nrow(data),]
X            <- matrix(1,nrow(Y),1)
N            <- ncol(Y)
K            <- 1+p*N
h            <- 20   # 20 quarters step ahead forecast (5 years)

for (i in 1:p){
  X          <- cbind(X,data[((p+1):nrow(data))-i,])
}

X            <- as.matrix(X)  # change from data frame to matrix
Y            <- as.matrix(Y)  # change from data frame to matrix
```

```{r MLE Empirical}
#| echo: false
#| message: false
#| warning: false
A.hat        <- solve(t(X)%*%X) %*%t(X) %*%Y                
Sigma.hat    <- t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y) 
```



The sign restriction that the author want to impose is applied to the contemporaneous shock or matrix $B_{0}^{-1}$ which is attached to the $u_{t}$ after transforming the SVAR model equation.


```{=tex}
\begin{align}
B_0y_t &= b_0 + \sum_{i=1}^{p}B_{i}y_{t-i}+u_t  \\

y_t &= B_0^{-1}b_0 + \sum_{i=1}^{p}B_0^{-1}B_{i}y_{t-i} + B_0^{-1}u_t

\end{align}
```


However, the SVAR model can be estimated after estimating the reduced form. To obtain the matrices $A$ and $\Sigma$ from the reduced form, the author uses the same technique as when dealing with the simulation data, which involves randomly drawing both matrices from the posterior distribution.

One of the changes is the prior value  of $\kappa_{1}$, which represents the overall shrinkage level of the autoregressive slope. Formerly, for the simulation data, the author used $0.02^{2}$, indicating very strong shrinkage. In this section, the author will loosen the shrinkage to 1 (no shrinkage).

<br>

The sign restriction for the monetary policy shock using Taylor's rule is as follows.


|  | cashrate | realgdp | CPI | unemprate | stockprice | govexp |
|--------------|----------|----------|------|----------|----------|----------|
| monetary policy shock | + | - | - |  |  |  |

A monetary policy shock that raises the cash rate target will cool down inflationary pressures (leading to a decrease in _CPI_) and also slow down economic growth (resulting in a decrease in _realgdp_).


<br>

As mentioned earlier, this study includes _govexp_ as a control variable for fiscal policy in the model. The sign restriction is imposed on _govexp_ itself, indicating an increase in government expenditure.


|  | cashrate | realgdp | CPI | unemprate | stockprice | govexp |
|--------------|----------|----------|------|----------|----------|----------|
| fiscal policy shock |  |  |  |  |  | + |

<br>

Or can be represented in the compact form as follows.


```{=tex}
\begin{bmatrix}
\text{cashrate} \\
\text{realgdp} \\
\text{CPI} \\
\text{unemprate} \\
\text{stockprice} \\
\text{govexp}
\end{bmatrix}
=
\underbrace{\begin{bmatrix}
    + && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? \\
    - && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? \\
    - && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? \\
    \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? \\
    \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? \\
    \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && \color{Grey}? && + \\
\end{bmatrix}}_{B_{0}^{-1}}
*
\begin{bmatrix}
{u_{t}^{monetary\:pol.}} \\
{\color{Grey}u_{\color{Grey}t}}^{\color{Grey}2} \\
{\color{Grey}u_{\color{Grey}t}}^{\color{Grey}3} \\
{\color{Grey}u_{\color{Grey}t}}^{\color{Grey}4} \\
{\color{Grey}u_{\color{Grey}t}}^{\color{Grey}5} \\
{u_{t}^{fiscal\:pol.}}
\end{bmatrix} 
```

```{r}
kappa.1 <- 1
kappa.2 <- 100 
```


Since the restriction on the simulated data has been changed, the new function can be represented as follows:


```{r}
#| echo: true
ImposeSignRestriction.empirical <- function (N,p,posterior.draws, S){
  
  A.posterior    <- posterior.draws$A.posterior
  Sigma.posterior<- posterior.draws$Sigma.posterior
  B0.tilde       <- posterior.draws$B0.tilde
  Bplus.tilde    <- posterior.draws$Bplus.tilde
  S              <- dim(A.posterior)[3]
  B0.store       <- array(NA,c(N,N,S))
  Bplus.store    <- array(NA,c(N,K,S))
  B0.inv.store   <- array(NA,c(N,N,S))
  
  i.vec = c()

  for (s in 1:S){
    B0.tilde1    <- B0.tilde[,,s]
    Bplus.tilde1 <- Bplus.tilde[,,s]
    
    i=1
    sign.restrictions.do.not.hold = TRUE 
    
    while (sign.restrictions.do.not.hold){
    X            <- matrix(rnorm(N*N),N,N)         
    QR           <- qr(X, tol = 1e-10)
    Q            <- qr.Q(QR,complete=TRUE)
    R            <- qr.R(QR,complete=TRUE)
    Q            <- t(Q %*% diag(sign(diag(R))))
    B0           <- Q%*%B0.tilde1                   
    Bplus        <- Q%*%Bplus.tilde1                  
    B0.inv       <- solve(B0)      
    check        <- (B0.inv[1, 1] > 0 & B0.inv[2, 1] < 0 & B0.inv[3, 1] < 0 & B0.inv[6, 6] > 0) >0
    if (check){sign.restrictions.do.not.hold=FALSE}
    i=i+1 
    }
    
    i.vec             <- c(i.vec,i) 
    B0.store[,,s]     <- B0
    Bplus.store[,,s]  <- Bplus
    B0.inv.store[,,s] <- B0.inv
  }
  
  return (list(B0.store     = B0.store[,,1:S],
               Bplus.store  = Bplus.store[,,1:S],
               B0.inv.store = B0.inv.store[,,1:S],
               i            = i.vec))
}

```

```{r estimating empirical basic model}
prior.parameters     <- Prior.Dist(N,p,Sigma.hat,kappa.1,kappa.2)
posterior.parameters <- Post.Dist(X,Y,prior.parameters)
posterior.draws      <- Post.Draw(N,S,p,posterior.parameters)
B.draws.restricted   <- ImposeSignRestriction.empirical(N,p,posterior.draws,S)
B0.inv.store         <- B.draws.restricted$B0.inv.store
Bplus.store          <- B.draws.restricted$Bplus.store
B0.inv.mean          <- apply(B0.inv.store,1:2,mean)
Bplus.mean           <- apply(Bplus.store,1:2,mean)
B0.inv.sd            <- apply(B0.inv.store, 1:2, sd)
```



## The baseline model

After imposing these restrictions on $B_{0}^{-1}$, the mean of $B_{0}^{-1}$ is displayed in Table 9, conforming to the sign restrictions in the $1^{st} - 3^{rd}$ rows of the  $1^{st}$ column and the $6^{th}$ row of the $6^{th}$ column, denoted as $B_{11},B_{21}, B_{31}$, and $B_{66}$ respectively.



```{r Show B0.inv.mean}
B0.inv_df           <- as.data.frame(B0.inv.mean)
B0.inv_df           <- round(B0.inv_df, digits = 4)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 21: Mean of the B0 inverse matrix")
```

```{r Show B0.inv.SD}
B0.inv_df           <- as.data.frame(B0.inv.sd)
B0.inv_df           <- round(B0.inv_df, digits = 4)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 22: SD of the B0 inverse matrix")
```

The trace plot and histogram of the restricted variable confirms that $B_{0}^{-1}$ shows $B_{11}$ as positive, $B_{21}$ and $B_{31}$ as negative, and $B_{66}$ as positive, consistent with the specified restrictions.

```{r}
par(mfrow = c(4, 2), mar = c(4, 4, 2, 1), oma = c(0, 2, 2, 2))
plot(B0.inv.store[1,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[11]), lwd = 0.05)
hist(B0.inv.store[1,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[11]))
plot(B0.inv.store[2,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[21]), lwd = 0.05)
hist(B0.inv.store[2,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[21]))
plot(B0.inv.store[3,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[31]), lwd = 0.05)
hist(B0.inv.store[3,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[31]))
plot(B0.inv.store[6,6,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[66]), lwd = 0.05)
hist(B0.inv.store[6,6,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[66]))
```

<div style="text-align: center;">  
##### Figure 14: The trace plot and histogram of B0 inverse (The baseline model)
</div>

### Impulse response function

The function for calculating 


```{r compute IRF function}
#| echo: true
compute_IRF <- function(h, S2, N, p) {
  
  A.posterior       <- posterior.draws$A.posterior
  B0_inv_store      <- B.draws.restricted$B0.inv.store
  IRF.posterior     <- array(NA, c(N, N, h + 1, S2))
  IRF.inf.posterior <- array(NA, c(N, N, S2))
  J                 <- cbind(diag(N), matrix(0, N, N * (p - 1)))
  
  for (s in 1:S2) {
    A.bold                      <- rbind(t(A.posterior[2:(1 + N * p), , s]), 
                                      cbind(diag(N * (p - 1)), matrix(0, N * (p - 1), N)))
    IRF.inf.posterior[, , s]    <- J %*% solve(diag(N * p) - A.bold) %*% t(J) %*% B0_inv_store[, , s]
    A.bold.power                <- A.bold
    
    for (i in 1:(h + 1)) {
      if (i == 1) {
        IRF.posterior[, , i, s] <- B0_inv_store[, , s]
      } else {
        IRF.posterior[, , i, s] <- J %*% A.bold.power %*% t(J) %*% B0_inv_store[, , s]
        A.bold.power            <- A.bold.power %*% A.bold
      }
    }
  }
  return(list(IRF.posterior     = IRF.posterior, 
              IRF.inf.posterior = IRF.inf.posterior))
}
```

```{r}
#| echo: false
#| message: false
#| warning: false
IRF               <- compute_IRF(h, S2, N, p)
IRF.1             <- IRF[["IRF.posterior"]]
IRFs.k1           <- apply(IRF.1[,1,,],1:2,median)
rownames(IRFs.k1) <- rownames(adf_matrix)
```

```{r}
#| echo: false
#| message: false
#| warning: false
library(HDInterval)
IRFs.k1.hdi       <- apply(IRF.1[,1,,],1:2,hdi, credMass=0.68)
hh                <- 1:(h+1)
```

```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(3,2), mar=c(4,4,2,2),cex.axis=1, cex.lab=1)

for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5| n==6){
    axis(1,c(1,5,9,13,17,21),c("1 qtr.","1 yr.","2 yrs.","3 yrs.","4 yrs.","5 yrs."))
  } else {
    axis(1,c(1,5,9,13,17,21),c("","","","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col="lightgoldenrod1",border="goldenrod2")
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2.5,col="gray26")
}
```

<div style="text-align: center;">  
##### Figure 15: The impulse response to the monetary shock (baseline model) 
</div>


<br>

The [Figure 14: The impulse response to the monetary shock (baseline model)] displays the response to the monetary shock. The light yellow shaded area represents the 68% confidence interval. In the short run to medium run, half year to three years after the shock, real GDP experiences a slight decline. Similarly, in the medium run, three to four years after the shock, the CPI also shows a slight decrease, and both variables exhibiting a persistent trend.

Regarding the cash rate itself, after rising from the shock, it tends to normalize after 2.5 years. The unemployment rate rises contemporaneously with the shock and persists even 5 years afterward. However, the stock price and government expenditure do not show any pronounced response. Unfortunately, for these four variables, including zero in the shaded area indicates that they are not significantly different from zero.


## The extension model



```{r estimation empirical extension model}
posterior.draws     <- Gibbsampler.Extension(Y, prior.parameters, posterior.parameters, S1,S2, N, K)
B.draws.restricted  <- ImposeSignRestriction.empirical(N,p,posterior.draws,S)
gamma.store         <- posterior.draws$gamma.store
B0.inv.store        <- B.draws.restricted$B0.inv.store
Bplus.store         <- B.draws.restricted$Bplus.store
B0.inv.mean         <- apply(B0.inv.store,1:2,mean)
Bplus.mean          <- apply(Bplus.store,1:2,mean)
B0.inv.sd           <- apply(B0.inv.store,1:2,sd)
```

```{r}
B0.inv_df           <- as.data.frame(B0.inv.mean)
B0.inv_df           <- round(B0.inv_df, digits = 4)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 23: Mean of the B0 inverse matrix (extension model)")
```

```{r}
B0.inv_df           <- as.data.frame(B0.inv.sd)
B0.inv_df           <- round(B0.inv_df, digits = 4)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 24: SD of the B0 inverse matrix (extension model)")
```


The mean value of $\gamma$ is equal `r gamma.mean`, which is close to zero. The plot and histogram shown in Figure 16 confirms that the draw oscillates around 0.0015.


```{r}
par(mfrow = c(1, 2))
plot(gamma.store,type='l',col="slategray4",ylab=expression(gamma),xlab="no. of iterations",main=expression("Plot of " * gamma), lwd = 0.1)
abline(h=gamma.mean,lty=3)
hist(gamma.store, breaks=500, border="steelblue",xlab=expression(gamma), main=expression("Histogram of " * gamma))
```

<div style="text-align: center;">  
##### Figure 16: The plot and histogram of gamma (The extension model)
</div>


```{r}
par(mfrow = c(4, 2), mar = c(4, 4, 2, 1), oma = c(0, 2, 2, 2))
plot(B0.inv.store[1,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[11]), lwd = 0.05)
hist(B0.inv.store[1,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[11]))
plot(B0.inv.store[2,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[21]), lwd = 0.05)
hist(B0.inv.store[2,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[21]))
plot(B0.inv.store[3,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[31]), lwd = 0.05)
hist(B0.inv.store[3,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[31]))
plot(B0.inv.store[6,6,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[66]), lwd = 0.05)
hist(B0.inv.store[6,6,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[66]))
```


<div style="text-align: center;">  
##### Figure 17: The trace plot and histogram of B0 inverse (The extension model)
</div>


### Impulse response function



```{r}
IRF               <- compute_IRF(h, S2, N, p)
IRF.1             <- IRF[["IRF.posterior"]]
IRFs.k1           <- apply(IRF.1[,1,,],1:2,median)
rownames(IRFs.k1) <- rownames(adf_matrix)
```

```{r}
library(HDInterval)
IRFs.k1.hdi    = apply(IRF.1[,1,,],1:2,hdi, credMass=0.68)
hh = 1:(h+1)
```

```{r}
par(mfrow=c(3,2), mar=c(4,4,2,2),cex.axis=1, cex.lab=1)

for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5| n==6){
    axis(1,c(1,5,9,13,17,21),c("1 qtr.","1 yr.","2 yrs.","3 yrs.","4 yrs.","5 yrs."))
  } else {
    axis(1,c(1,5,9,13,17,21),c("","","","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col="lightgoldenrod1",border="goldenrod2")
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2.5,col="gray26")
}


```

<div style="text-align: center;">  
##### Figure 18: The impulse response to the monetary shock (extension model) 
</div>


<br>

However, the [Figure 18: The impulse response to the monetary shock (extension model)] displays the impulse response slightly differently from what it represents in Figure 15, the baseline model. The CPI tends to decrease after 2.5 year of the shock and continues declining even after 5 years without a chance to recover. Furthermore, real GDP is observed to decrease from half a year up to around 5 years after the shock. Moreover, the unemployment rate significantly increase a little bit during after 3 - 5 years after the shock.

As for the cash rate, stock price, and government expenditure, there isn't much different from the previous model. All of these variables remain not significantly different from zero.



## The baseline with Stochastic volatillity model


```{r}
prior.parameters.stochastic <- Prior.Dist(N,p,Sigma.hat,kappa.1,kappa.2)
posterior.draws        <- Post.draws.stochastic(Y,X,prior.parameters.stochastic,S1,S2)
B.draws.restricted     <- ImposeSignRestriction.empirical(N,p,posterior.draws)
B0.inv.store           <- B.draws.restricted$B0.inv.store
Bplus.store            <- B.draws.restricted$Bplus.store
B0.inv.mean            <- apply(B0.inv.store,1:2,mean)
Bplus.mean             <- apply(Bplus.store,1:2,mean)
B0.inv.sd              <- apply(B0.inv.store,1:2,sd)
```

```{r Show B0.inv.mean stochastic}
B0.inv_df <- as.data.frame(B0.inv.mean)
B0.inv_df <- round(B0.inv_df, digits = 4)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 25: Mean of the B0 inverse matrix")
```

```{r Show B0.inv.sd stochastic}
B0.inv_df <- as.data.frame(B0.inv.sd)
B0.inv_df <- round(B0.inv_df, digits = 4)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 26: SD of the B0 inverse matrix")
```

```{r}
par(mfrow = c(4, 2), mar = c(4, 4, 2, 1), oma = c(0, 2, 2, 2))
plot(B0.inv.store[1,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[11]), lwd = 0.05)
hist(B0.inv.store[1,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[11]))
plot(B0.inv.store[2,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[21]), lwd = 0.05)
hist(B0.inv.store[2,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[21]))
plot(B0.inv.store[3,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[31]), lwd = 0.05)
hist(B0.inv.store[3,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[31]))
plot(B0.inv.store[6,6,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[66]), lwd = 0.05)
hist(B0.inv.store[6,6,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[66]))
```


<div style="text-align: center;">  
##### Figure 19: The trace plot and histogram of B0 inverse (The basic with stochastic volatility model)
</div>


The posterior draw mean of $\sigma^{2}$ for each period is displayed in Figure 20. The $\sigma^{2}$ moves around 8000. We can also see that the $\sigma^{2}$ rises up to 60000 in some particular periods.


```{r}
sigma2.mean <- apply(posterior.draws$H.sv,1,mean) 
plot.ts(sigma2.mean,main=expression(paste("Posterior Mean of ",sigma^2)),
        ylab="",type='l',lwd=2,col="tomato4",xlab="T")
abline(h=mean(sigma2.mean),lty=2)
```

<div style="text-align: center;">  
##### Figure 20: The posterior mean of the stochastic volatility (The baseline with stochastic volatility model)
</div>


### Impulse response function


```{r}
IRF <- compute_IRF(h, S2, N, p)
IRF.1 <- IRF[["IRF.posterior"]]
IRFs.k1           = apply(IRF.1[,1,,],1:2,median)
rownames(IRFs.k1) = rownames(adf_matrix)
```

```{r}
library(HDInterval)
IRFs.k1.hdi    = apply(IRF.1[,1,,],1:2,hdi, credMass=0.68)
hh = 1:(h+1)
```

```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(3,2), mar=c(4,4,2,2),cex.axis=1, cex.lab=1)

for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5| n==6){
    axis(1,c(1,5,9,13,17,21),c("1 qtr.","1 yr.","2 yrs.","3 yrs.","4 yrs.","5 yrs."))
  } else {
    axis(1,c(1,5,9,13,17,21),c("","","","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col="lightsalmon2",border="lightsalmon4")
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2.5,col="gray26")
}


```

<div style="text-align: center;">  
##### Figure 21: The impulse response to the monetary shock (the baseline with stochastic volatility model) 
</div>


The [Figure 21: The impulse response to the monetary shock (the baseline with stochastic volatility model)] displays the impulse response differently from how it is represented in the previous two models. We cannot observe the cash rate normalizing around three years after the shock. Moreover, the CPI decreases at the same time as the monetary shock. However, it exhibits an increasing trend, tending to recover five years after the shock. Additionally, we cannot observe the unemployment rate increase that was seen at the same time in the previous two models. All of these variables remain not significantly different from zero.




## The extension with Stochastic volatillity model


```{r Estimate extension and stochastic model for empirical}

prior.parameters.stochastic <- Prior.Dist(N,p,Sigma.hat,kappa.1,kappa.2)
posterior.draws <- Post.draws.stochastic.extension(Y,X,prior.parameters.stochastic,S1,S2)
B.draws.restricted <- ImposeSignRestriction.empirical(N,p,posterior.draws)
B0.inv.store <- B.draws.restricted$B0.inv.store
Bplus.store <- B.draws.restricted$Bplus.store
B0.inv.mean <- apply(B0.inv.store,1:2,mean)
Bplus.mean <- apply(Bplus.store,1:2,mean)
gamma.store  <- posterior.draws$gamma.store
gamma.mean   <- round(mean(gamma.store), digits = 4)
B0.inv.sd <- apply(B0.inv.store,1:2,sd)
```

```{r Show B0.inv.mean stochastic extension}
B0.inv_df <- as.data.frame(B0.inv.mean)
B0.inv_df <- round(B0.inv_df, digits = 7)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 27: Mean of the B0 inverse matrix")
```

```{r Show B0.inv.sd stochastic extension}
B0.inv_df <- as.data.frame(B0.inv.sd)
B0.inv_df <- round(B0.inv_df, digits = 7)
colnames(B0.inv_df) <- c("C1", "C2", "C3", "C4", "C5", "C6")
rownames(B0.inv_df) <- c( "R1", "R2", "R3", "R4", "R5", "R6")
knitr::kable(B0.inv_df, caption = "Table 28: SD of the B0 inverse matrix")
```

```{r}
par(mfrow = c(4, 2), mar = c(4, 4, 2, 1), oma = c(0, 2, 2, 2))
plot(B0.inv.store[1,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[11]), lwd = 0.05)
hist(B0.inv.store[1,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[11]))
plot(B0.inv.store[2,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[21]), lwd = 0.05)
hist(B0.inv.store[2,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[21]))
plot(B0.inv.store[3,1,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[31]), lwd = 0.05)
hist(B0.inv.store[3,1,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[31]))
plot(B0.inv.store[6,6,],type='l',col="slategray4", ylab="",xlab="", main=expression(B[66]), lwd = 0.05)
hist(B0.inv.store[6,6,], breaks=500, border="steelblue",xlab="", main=expression("Histogram of " * B[66]))
```


<div style="text-align: center;">  
##### Figure 22: The trace plot and histogram of B0 inverse (The basic with stochastic volatility model)
</div>

The mean value of $\gamma$ is equal `r gamma.mean`, which is close to zero and also almost the same as before the author incorporates the stochastic volatility. The plot and histogram shown in Figure 22.



```{r}
par(mfrow = c(1, 2))
plot(gamma.store,type='l',col="slategray4",ylab=expression(gamma),xlab="no. of iterations",main=expression("Plot of " * gamma), lwd = 0.1)
abline(h=gamma.mean,lty=3)
hist(gamma.store, breaks=500, border="steelblue",xlab=expression(gamma), main=expression("Histogram of " * gamma))
```


<div style="text-align: center;">  
##### Figure 23: The plot and histogram of gamma (The extension with stochastic volatility model)
</div>


The posterior draw mean of $\sigma^{2}$ for each period is displayed in Figure 24. The $\sigma^{2}$ is significantly higher compared to the previous model.



```{r}
par(mfrow=c(1, 1))
sigma2.mean <- apply(posterior.draws$H.sv,1,mean) 
plot.ts(sigma2.mean,main=expression(paste("Posterior Mean of ",sigma^2)),
        ylab="",type='l',lwd=2,col="tomato4",xlab="T")
abline(h=mean(sigma2.mean),lty=2)
```

<div style="text-align: center;">  
##### Figure 24: The posterior mean of the stochastic volatility (The extension with stochastic volatility model)
</div>







### Impulse response function

However, the Figure 25 The impulse response to the monetary shock (the extension with stochastic volatility model), displays the point estimate impulse response as a flat line, indicating a value actually close to zero. The 68% confidence interval is also flat. We cannot observe any significant effect on all six variables in the system. One possible explanation is that we might need more observations to capture the heteroskedasticity, which, in this case, is not sufficient.


```{r}
IRF               <- compute_IRF(h, S2, N, p)
IRF.1             <- IRF[["IRF.posterior"]]
IRFs.k1           = apply(IRF.1[,1,,],1:2,median)
rownames(IRFs.k1) = rownames(adf_matrix)
```

```{r}
library(HDInterval)
IRFs.k1.hdi    = apply(IRF.1[,1,,],1:2,hdi, credMass=0.68)
hh = 1:(h+1)
```

```{r}
#| echo: false
#| message: false
#| warning: false
par(mfrow=c(3,2), mar=c(4,4,2,2),cex.axis=1, cex.lab=1)

for (n in 1:N){
  ylims     = range(IRFs.k1[n,hh],IRFs.k1.hdi[,n,hh])
  plot(hh,IRFs.k1[n,hh], type="l", ylim=ylims, axes=FALSE, xlab="", ylab=rownames(IRFs.k1)[n])
  if (n==5| n==6){
    axis(1,c(1,5,9,13,17,21),c("1 qtr.","1 yr.","2 yrs.","3 yrs.","4 yrs.","5 yrs."))
  } else {
    axis(1,c(1,5,9,13,17,21),c("","","","","",""))
  }
  axis(2,c(ylims[1],0,ylims[2]),round(c(ylims[1],0,ylims[2]),3))
  polygon(c(hh,(h+1):1), c(IRFs.k1.hdi[1,n,hh],IRFs.k1.hdi[2,n,(h+1):1]), col="lightsalmon2",border="lightsalmon4")
  abline(h=0)
  lines(hh, IRFs.k1[n,hh],lwd=2.5,col="gray26")
}


```

<div style="text-align: center;">  
##### Figure 25: The impulse response to the monetary shock (the extension with stochastic volatility model) 
</div>


## References {.unnumbered}

