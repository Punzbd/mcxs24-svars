---
title: "The Impact of Monetary Policy on the Real Economy and the Stock Market: The Case of Australia"
author: "Pun Suparattanapinan"

execute:
  echo: false
  
bibliography: references.bib
---

> **Abstract.** The purpose of this study is to investigate the relationship and impact of monetary policy on the real economy in Australia. The author uses quarterly data spanning from 1990 Q1 to 2023 Q4. The Structural Vector Autoregression (SVAR) model is used to capture the structural relationship, including analyzing the impulse responses of the variable of interest to structural shock. 
>
> **Keywords.** Structural VAR, SVAR, monetary policy, stock market, economic impact, impulse response function, Bayesian, cash rate

# Introduction

This study aims to explore the dynamic impact of monetary policy on the real economy in Australia, specifically focusing on economic growth, prices, and employment. How does the real economy respond? What is the magnitude and the duration of change? The stock market price is included and can be considered a leading indicator of the real economy.

According to Economic theory, the transmission mechanism of monetary policy to the real economy takes time, as it does not have a fully immediate effect, but it involves some delays. @BrischettoVoss1999 find that the contractionary of monetary policy in Australia leads to a decrease in output level between 5 and 15 quarters after the contraction. The price level also gradually falls with some delay with maximum effect, and there is an indication that the effect tends to be permanent. @Mojon2001 also investigate the effect of monetary policy across 10 euro area countries and find that a contractionary monetary policy shock leads to a temporary fall in GDP that peaks typically around four quarters after the shock and a gradual decrease in the price level.

On the other hand, @Bjornland2009 suggest that, due to the availability of information in financial market, the monetary policy and stock market have simultaneous effects. They also find that real stock prices immediately fall by seven to nine percent due to a monetary policy shock that raises the federal funds rate by 100 basis points. Similarly, @Ioannidis2008 find that the majority of OECD countries under study, periods of tight money are associated with contemporaneous declines in stock market value and also decreases expected stock returns. Additionally, according to @Bjornland2009, the changes in asset price, particularly stock price, ultimately impact the economic growth by increasing household spending based on their wealth and encouraging investment through the Tobin Q effect. The asset prices also play a role in firmâ€™s ability to fund operations through credit channel. Hence, policymakers are motivated to track the asset price as short-run indicators.

Understanding insights of these dynamic impacts within the macroeconomy, including the magnitude and duration of impacts, enables the policymakers to precisely predict the outcomes on real economy that might occur after changing the new cash target rate.


# Data

The variables in this study include the cash rate target **_(cashrate)_**, which indicates the monetary policy. GDP (in real terms), CPI, and the unemployment rate **_(unemprate)_** are included to measure the real economy. For the stock market, the author uses the All Ordinaries index (AORD) as a representative. The AORD is the market-weighted index and includes about 500 companies from the Australian Stock Exchange. Finally, total government expenditure is included to control for fiscal policy, which might also affect the economy besides monetary policy. The data is collected from the Reserve Bank of Australia (RBA), Australian Bureau of Statistics (ABS), and Yahoo Finance. The dataset spans from 1990 Q1 to 2023 Q4, comprising 136 observations. [Figure 1: time series plots (raw data)] represents the raw data time series of these 6 variables.

<br>


```{r download the data}
#| message: false
#| warning: false

  # Unemployment rate (02/1978 - 02/2024) monthly data
unemp_rate_raw <- readrba::read_rba(series_id = "GLFSURSA")
unemp_rate <- unemp_rate_raw[, c("date", "value")]
unemp_rate <- xts::xts(unemp_rate$value,unemp_rate$date)
    # we fix the period (1990Q1 - 2023Q4)    
unemp_rate <- xts::to.quarterly(unemp_rate, OHLC = FALSE)
unemprate <- unemp_rate[zoo::index(unemp_rate) >= "1990 Q1" & zoo::index(unemp_rate) < "2024 Q1"]

  # GDP deflator (09/1959 - 12/2023) quarterly
gdp_df_raw <- readabs::read_abs(series_id = "A2303730T")
gdp_df <- gdp_df_raw[, c("date", "value")]
gdp_df$quarter <- zoo::as.yearqtr(gdp_df$date)
gdp_df <- xts::xts(gdp_df$value,gdp_df$quarter)
    # we fix the period (1990Q1 - 2023Q4)  
gdp_df <- gdp_df[zoo::index(gdp_df) >= "1989 Q4" & zoo::index(gdp_df) < "2024 Q1"]

  # Real GDP seasonal adjusted
real_gdp_raw <- readrba::read_rba(series_id = "GGDPCVGDP")
real_gdp <- real_gdp_raw[, c("date", "value")]
real_gdp$quarter <- zoo::as.yearqtr(real_gdp$date)
real_gdp <- xts::xts(real_gdp$value,real_gdp$quarter)
    # we fix the period (1990Q1 - 2023Q4)   
realgdp <- real_gdp[zoo::index(real_gdp) >= "1990 Q1" & zoo::index(real_gdp) < "2024 Q1"]
realgdp <- realgdp/1000
lnrealgdp <- log(realgdp)
    # calculate gdp growth
#realgdp_temp <- real_gdp[zoo::index(real_gdp) >= "1989 Q4" & zoo::index(real_gdp) < "2024 Q1"]
#drealgdp <- 100*diff(log(realgdp_temp))
#drealgdp <- drealgdp[2:137]


  # Cash target rate (01/1990 - 03/2024) daily
i_raw <- readrba::read_rba(series_id = "FIRMMCRTD")
i <- i_raw[, c("date", "value")]
i <- xts::xts(i$value,i$date)
    # Convert to quarter and fix the period (1990Q1 - 2023Q4)
cashrate <- xts::to.quarterly(i, OHLC = FALSE)[1:136]


  # CPI (06/1922 - 12/2023) quarterly
CPI_raw <- readrba::read_rba(series_id = "GCPIAG")
CPI <- CPI_raw[, c("date", "value")]
CPI$quarter <- zoo::as.yearqtr(CPI$date)
CPI <- xts::xts(CPI$value,CPI$quarter)
    # we fix the period (1989Q4 - 2023Q4)
CPI <- CPI[zoo::index(CPI) >= "1990 Q1" & zoo::index(CPI) < "2024 Q1"]
lnCPI <- log(CPI)
  # Inflation (Diff CPI) (1990Q1 - 2023Q4)
#inf <- 100*diff(log(CPI))
#inflation <- inf[2:137]

  # Stock market (01/1985 - 03/2024) monthly
link_AORD <- "https://query1.finance.yahoo.com/v7/finance/download/%5EAORD?period1=460339200&period2=1711843200&interval=1mo&events=history&includeAdjustedClose=true"
AORD_raw <- read.csv(link_AORD)
AORD <- AORD_raw[, c("Date", "Close")]
AORD <- xts::xts(AORD$Close,as.Date(AORD$Date))
    # Convert to quarter and fix the period (1990Q1 - 2023Q4) and convert to ln term
AORD <- xts::to.quarterly(AORD, OHLC = FALSE)
stockprice <- AORD[zoo::index(AORD) >= "1990 Q1" & zoo::index(AORD) < "2024 Q1" ]
lnstockprice <- log(stockprice)
    # calculate the growth of stock price
#stockprice_temp <- AORD[zoo::index(AORD) >= "1989 Q4" & zoo::index(AORD) < "2024 Q1" ]
#stockprice_r <- stockprice_temp / gdp_df
#dstockprice <- 100*diff(log(stockprice_r))
#dstockprice <- dstockprice[2:137]

  # Government spending (Final consumption national + state and local + seasonal adj) (09/1959 - 12/2023) quarterly
gov_raw <- readabs::read_abs(series_id = "A2304036K")
gov <- gov_raw[, c("date", "value")]
gov$quarter <- zoo::as.yearqtr(gov$date)
gov <- xts::xts(gov$value,gov$quarter)
    # we fix the period (1990Q1 - 2023Q4) and convert to real term, ln term
gov_exp <- gov[zoo::index(gov) >= "1990 Q1" & zoo::index(gov) < "2024 Q1"]
ln_gov_exp<- log(gov_exp)

    # calculate the growth of government spending
#gov_exp_temp <- gov[zoo::index(gov) >= "1989 Q4" & zoo::index(gov) < "2024 Q1"]
#gov_exp_r <- gov_exp_temp/(gdp_df*1000)
#dgovexp <- 100*diff(log(gov_exp_r))
#dgovexp <- dgovexp[2:137]
```


```{r plotting the raw data}

time <- seq(as.Date("1990-01-01"), by = "quarter", length.out = 136)
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))
par(mar=c(3,3,2,2))

plot(time, realgdp, type = "l", lwd = 2 ,col = "grey27", main = "Real GDP ($billion)")
plot(time, cashrate, type = "l", lwd = 2 ,col = "grey27", main = "Cash rate target (%)")
plot(time, CPI, type = "l", lwd = 2 ,col = "grey27", main = "CPI")
plot(time, unemprate, type = "l", lwd = 2 ,col = "grey27", main = "Unemployment rate (%)")
plot(time, stockprice, type = "l", lwd = 2 , col = "grey27", main = "Stock market index (All ordinaries)")
plot(time, gov_exp, type = "l", lwd = 2 ,col = "grey27", main = " Total government expenditure ($ billion)")

#mtext("Plot of variables (before transforming)", outer = TRUE, cex = 1.1, font = 2, col = "darkslateblue")

```
<div style="text-align: center;">  
##### Figure 1: time series plots (raw data) 
</div>

<br>

The author transforms the data in manner that aligns with the purpose of study. The total government expenditure are adjusted to real terms using the GDP deflator. After that, the author transforms 4 variables which are real GDP, AORD, CPI, and real government expenditure in logarithms term, denoted by **_realgdp_**, **_stockprice_**, **_CPI_**, and **_govexp_** respectively. [Figure 2: time series plots (transformed data)] represents the transformed data, which will be analyzed further.

<br>

```{r Plotting the transformed data}

time <- seq(as.Date("1990-01-01"), by = "quarter", length.out = 136)


par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))

options(repr.plot.width=20, repr.plot.height=50)
par(mar=c(3,3,2,2))

plot(time, lnrealgdp, type = "l", lwd = 2 ,col = "grey27", main = "ln Real GDP")
plot(time, cashrate, type = "l", lwd = 2 ,col = "grey27", main = "Cash rate target (%)")
plot(time, lnCPI, type = "l", lwd = 2 ,col = "grey27", main = "ln CPI")
plot(time, unemprate, type = "l", lwd = 2 ,col = "grey27", main = "Unemployment rate (%)")
plot(time, lnstockprice, type = "l", lwd = 2 , col = "grey27", main = "ln stock market index price")
plot(time, ln_gov_exp, type = "l", lwd = 2 ,col = "grey27", main = "ln total government expenditure growth")

#mtext("Plot of 6 variables (after transforming)", outer = TRUE, cex = 1.1, font = 2, col = "darkslateblue")
```

<div style="text-align: center;"> 
##### Figure 2: time series plots (transformed data) 
</div>




<br>

The statistics summary of variables from 1990 Q1 to 2023 Q4 is shown in Table 1.


```{r statistical summary}
data = data.frame(cashrate, lnrealgdp, lnCPI, unemprate, lnstockprice, ln_gov_exp)

summary_stats <- function(x) {
  c(
    N = length(x),
    Mean = round(mean(x, na.rm = TRUE),3),
    St.Dev. = round(sd(x, na.rm = TRUE),3),
    Min = round(min(x, na.rm = TRUE),3),
    Max = round(max(x, na.rm = TRUE),3)
  )
}

result <- sapply(data, summary_stats)

result_df <- as.data.frame(t(result))
colnames(result_df) <- c("N", "Mean", "St.Dev.", "Min", "Max")
rownames(result_df) <- c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")

#options(width = 200)

knitr::kable(result_df, caption = "Table 1: Summary statistics")
```





# Preliminary Results
### The autocorrelation and partial autocorrelation
In this section, the autocorrelation and partial autocorrelation analyses are used to detect patterns and check the randomness of time series. [Figure 3: ACF plots] show the autocorrelation remains in all variables, even after 5 years. This indicate the strong correlation between a time series and its lagged values.



```{r ACF plot}
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))
par(mar=c(4,4,4,2))

# acf(cashrate, main = "cashrate")
# acf(drealgdp, main = "drealgdp")
# acf(inflation, main = "inflation")
# acf(unemprate, main = "unemprate")
# acf(dstockprice, main = "dstockprice")
# acf(dgovexp, main = "dgovexp")

for (i in 1:ncol(data)) {
  acf(data[, i], main = c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")[i])
}
```
<div style="text-align: center;"> 
##### Figure 3: ACF plots
</div>
<br>

For the PACF in [Figure 4: PACF plots], it is observed that there have not been any significant spikes  for all variables, except for _unemprate_. The PACF of _unemprate_ indicates significance in partial autocorrelation at first and sixteenth quarter lags; however, this might occur due to a type I error.
```{r PACF plot}
par(mfrow = c(3, 2), oma = c(0, 0, 2, 0))
par(mar=c(4,4,4,2))
# pacf(cashrate, main = "cashrate")
# pacf(drealgdp, main = "drealgdp")
# pacf(inflation, main = "inflation")
# pacf(unemprate, main = "unemprate")
# pacf(dstockprice, main = "dstockprice")
# pacf(dgovexp, main = "dgovexp")

for (i in 1:ncol(data)) {
  pacf(data[, i], main = c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")[i])
}
```

<div style="text-align: center;"> 
##### Figure 4: PACF plots
</div>
<br>

### The unit root test
In this section, the Augmented Dickey-Fuller Test (ADF Test) is used to test for the stationarity assumption. The null hypothesis is that the time series data has a unit root (non-stationary). The results are shown in Table 2. At the 1% significance level, the null hypothesis cannot be rejected for all variables, indicating that these variables are unit root non-stationary.



```{r ADF test}
#| echo: false
#| message: false
#| warning: false
adf_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
rownames(adf_matrix) <- c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp")
colnames(adf_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in seq_along(data)) {
  result <- tseries::adf.test(data[[i]])
  
  # Store the results in the 'adf' matrix
  adf_matrix[i, 1] <- round(as.numeric(result[1]), 2)  # ADF statistic
  adf_matrix[i, 2] <- result[2]  # p-value
  adf_matrix[i, 3] <- round(as.numeric(result[4]), 2)  # critical values
}

knitr::kable(adf_matrix, caption = "Table 2: ADF test")
```



Then, take the first difference and the second difference of all variables and rerun the ADF test. The results are shown in Table 3 and Table 4. For the first difference in Table 3, at the 1% significance level, the null hypothesis can be rejected for all variables, except _CPI_ and _dgovexp_. It is reasonable to conclude that _cashrate_, _realgdp_, _unemprate_, _stockprice_ are integrated of order 1. And _CPI_ and _govexp_ are integrated of order 2, as shown in the result in Table 4.


```{r create the first difference data}
#| echo: false
#| message: false
#| warning: false
#variables <- c("cashrate", "drealgdp", "inflation", "unemprate", "dstockprice", "dgovexp")

data_diff1 <- as.data.frame(lapply(colnames(data), function(x) na.omit(diff(get(x)))))

colnames(data_diff1) <- paste0("d", c("cashrate", "realgdp", "CPI", "unemprate", "stockprice", "govexp"))

```



```{r ADF test on first diff}
#| echo: false
#| message: false
#| warning: false
adf_diff_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
rownames(adf_diff_matrix) <- colnames(data_diff1)
colnames(adf_diff_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in seq_along(data)) {
  result_diff <- tseries::adf.test(data_diff1[[i]])
  
  # Store the results in the 'adf' matrix
  adf_diff_matrix[i, 1] <- round(as.numeric(result_diff[1]), 2)  # ADF statistic
  adf_diff_matrix[i, 2] <- result_diff[2]  # p-value
  adf_diff_matrix[i, 3] <- round(as.numeric(result_diff[4]), 2)  # critical values
}

knitr::kable(adf_diff_matrix, caption = "Table 3: ADF test on the first difference")
```

```{r create the second difference data}
#| echo: false
#| message: false
#| warning: false
#variables <- c("cashrate", "drealgdp", "inflation", "unemprate", "dstockprice", "dgovexp")

data_diff2 <- as.data.frame(lapply(data_diff1, function(x) diff(x)))

colnames(data_diff2) <- paste0("d", colnames(data_diff2))

```


```{r ADF test on second diff}
#| echo: false
#| message: false
#| warning: false
adf_diff_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
rownames(adf_diff_matrix) <- colnames(data_diff2)
colnames(adf_diff_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

for (i in seq_along(data)) {
  result_diff <- tseries::adf.test(data_diff2[[i]])
  
  # Store the results in the 'adf' matrix
  adf_diff_matrix[i, 1] <- round(as.numeric(result_diff[1]), 2)  # ADF statistic
  adf_diff_matrix[i, 2] <- result_diff[2]  # p-value
  adf_diff_matrix[i, 3] <- round(as.numeric(result_diff[4]), 2)  # critical values
}

knitr::kable(adf_diff_matrix, caption = "Table 4: ADF test on the second difference")
```


```{r}
#| echo: false
#| message: false
#| warning: false
### The unit root test without differencing
#Y = data.frame(cashrate, realgdp, inflation, unemprate, stockprice, gov_exp)

#adf_diff_matrix <- as.data.frame(matrix(nrow=6,ncol=3,NA))
#rownames(adf_diff_matrix) <- colnames(Y)
#colnames(adf_diff_matrix) <- c("Dickey-Fuller","Lag order", "p-value")

#for (i in seq_along(Y)) {
 #result_diff <- tseries::adf.test(Y[[i]])
  
  # Store the results in the 'adf' matrix
 # adf_diff_matrix[i, 1] <- round(as.numeric(result_diff[1]), 2)  # ADF statistic
 # adf_diff_matrix[i, 2] <- result_diff[2]  # p-value
 # adf_diff_matrix[i, 3] <- round(as.numeric(result_diff[4]), 2)  # critical values
#}

#knitr::kable(adf_diff_matrix)
```



# Methodology
In this section, the model that this study use to capture the structural relationship between real economy variables and the stock price is introduced. 
The author use Structural Vector Autoregression (SVAR) model which comprises of 6 variables, _cashrate_, _realgdp_, _inflation_, _unemprate_, _stockprice_, and _govexp_. Subsequently, the model will be used to analyze the impulse responses.



The **SVAR model** can be shown as follows.
```{=tex}
\begin{align}
B_0y_t &= b_0 + \sum_{i=1}^{p}B_{i}y_{t-i}+u_t  \\
u_t|Y_{t-1}&\sim iid \mathcal{N} (0_N,I_N)
\end{align}
```
where :

$y_{t}$ is $N\times1$ vector of endogenous variables at time t

$B_0$ is $N\times N$ structural matrix which captures the contemporaneous relationships between variables

$u_t$ is $N\times1$ vector of conditionally on $Y_{t-1}$ orthogonal or independent structural shocks

<br>

Specifically, $y_{t}$ contains 6 variables as follows.
$$
y_t = \begin{pmatrix}
\text{cashrate} \\
\text{realgdp} \\
\text{CPI} \\
\text{unemprate} \\
\text{stockprice} \\
\text{govexp}
\end{pmatrix}
$$


The **reduced form** can be shown as follows.

```{=tex}
\begin{align}
y_t &= \mu_0 + \sum_{i=1}^{p}A_{i}y_{t-i}+\epsilon_t \\
\epsilon_t|Y_{t-1} &\sim iid \mathcal{N}(0_N,\Sigma)
\end{align}
```



where :

$A_i$ is $N\times N$ matrix of autoregressive slope parameters

$\mu_0$ is $N\times1$ vector of constant terms

$\epsilon_t$ is $N\times1$ vector of error terms - a multivariate white noise process

$\Sigma$ is $N\times N$ covariance matrix of the error term























# Estimation
## The baseline model
### Estimation algorithm
 
The reduced form above can be rewrite in the matrix form as follows:
```{=tex}
\begin{align}
Y &= XA + E \\
E|X &\sim \mathcal{MN}_{T \times N}(0,\Sigma,I_T)
\end{align}
```


Bayes' rule is employed for deriving the posterior distribution

From Bayes' rule:
```{=tex}
\begin{align}
P(A,\Sigma|Y,X) &\propto L(A,\Sigma|Y,X)P(A,\Sigma) \\
\underbrace{P(A,\Sigma|Y,X)}_{\text{Posterior distribution}} &\propto \underbrace{L(A,\Sigma|Y,X)}_{\text{Likelihood}} \underbrace{P(A|\Sigma)P(\Sigma)}_{\text{Prior distribution}}
\end{align}
```
<br>


#### The likelihood fuction
The derivation of the likelihood function can be shown in matrix-variate normal distribution as follows:
```{=tex}
\begin{align}
L(A,\Sigma|Y,X) = (2\pi)^{-\frac{NT}{2}} det(\Sigma)^{-\frac{T}{2}}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-XA)'(Y-XA) \right] \right\} \\
\end{align}
```

Then, the $\hat{A}$ and $\hat{\Sigma}$ corresponding to the maximum likelihood are
```{=tex}
\begin{align}
\hat{A} &= (X'X)^{-1}X'Y \\
\hat{\Sigma} &= \frac{1}{T} (Y-X \hat{A})'(Y-X \hat{A})
\end{align}
```
  
The likelihood function can be represented as Normal inverse Wishart distribution 
```{=tex}
\begin{align}
L(A,\Sigma|Y,X) &\propto det(\Sigma)^{-\frac{T}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(Y-\hat{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
\end{align}
```
<br>

#### The prior distribution 

```{=tex}
\begin{align}
P(A,\Sigma) &= P(A|\Sigma) P(\Sigma) \\
A|\Sigma &\sim \mathcal{MN}_{K \times N} (\underline{A}, \Sigma , \underline{V}) \\
\Sigma &\sim \mathcal{IW}_{N}(\underline{S},\underline{\nu})
\end{align}
```
The prior which follows Normal inverse Wishart distribution:
```{=tex}
\begin{align}
p(A,\Sigma) &\propto det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\underline{S} \right] \right\}
\\
\end{align}
```


The Minnesota prior used as prior of the model, the parameters are follows:
```{=tex}
\begin{align}
\underline{A} &= [0_{N \times 1} \quad I_N \quad 0_{N \times (p-1)N}]' \\ 
Var[vec(A)] &= \Sigma \otimes  \underline{V} \\
\underline{V} &= \text{diag}([\kappa_2 \quad \kappa_1 (p^{-2} \otimes \imath_N)]) \\
\end{align}
```
where: 

$p$ = [1,2,...,p]

$\imath_N$ = [1,...,1]

$\kappa_1$ is overall shrinkage level for autoregressive slopes, the common value is $0.02^{2}$

$\kappa_2$ is overall shrinkage level for the constant term, the common value is $100$

```{=tex}
\begin{align}
P(A,\Sigma) &\propto det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\underline{S} \right] \right\}
\\
\end{align}
```
<br>
#### The posterior distribution 
From the Bayes' rule mentioned above, the posterior distribution as follows:

```{=tex}
\begin{align}
P(A,\Sigma|Y,X) \propto & L(A,\Sigma|Y,X)P(A,\Sigma) \\
P(A,\Sigma|Y,X) \propto 
& det(\Sigma)^{-\frac{T}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
&det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\underline{S} \right] \right\} \\
P(A,\Sigma|Y,X) \propto 
& det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}}
exp \left\{-\frac{1}{2} tr \left[
\Sigma^{-1}\left[(A-\hat{A})'X'X(A-\hat{A}) + (A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \\
+(Y-X\hat{A})'(Y-X\hat{A})+\underline{S})\right] \right] \right\}

\\

\end{align}
```

Which can represented in form of Normal inverse Wishart distribution
```{=tex}
\begin{align}
P(A,\Sigma|Y,X) &= P(A|Y,X,\Sigma) P(\Sigma|Y,X) \\
P(A|Y,X,\Sigma) &\sim \mathcal{MN}_{K \times N} (\overline{A}, \Sigma , \overline{V}) \\
P(\Sigma|Y,X) &\sim \mathcal{IW}_{N}(\overline{S},\overline{\nu})
\end{align}
```

<br>
And the posterior parameters are structured as follows.
```{=tex}
\begin{align}

\overline{V} &= (X'X+ \underline{V}^{-1})^{-1} \\ 
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1} \underline{A})\\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \underline{S}+Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - 
\overline{A}'\overline{V}^{-1}\overline{A} \\

\end{align}
```

### Data simulation

The author generates 1,000 observations from bi-variate Gaussian random walk process to prove that the algorithm works. 

```{r data simulation}
set.seed(2024)
RW1 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)
RW2 <- arima.sim(model= list(order = c(0, 1, 0)), n=1000, mean=0, sd=1)

RW <- cbind(RW1,RW2)
Y <- RW[2:nrow(RW),]

```

```{r plot random walk}
plot.ts(Y,main = "1,000 simulations of bi-variate Gaussian random walk", xlab = "Observations", col = "grey27", lwd = 2, frame.plot = FALSE)
```
The target model for estimation is a Sign-Restricted Structural Vector Autoregression (SVAR).

```{=tex}
\begin{align}
B_0y_t &= b_0 + \sum_{i=1}^{p}B_{1}y_{t-1}+u_t  \\
u_t|Y_{t-1}&\sim iid \mathcal{N} (0_N,I_N)
\end{align}
```


```{r Set up variables}
#| echo: false
#| message: false
#| warning: false
X            = matrix(1,nrow(Y),1)
X            = cbind(X,RW[2: nrow(RW)-1,])
N            = 2
p            = 1
K = 1+p*N
```


```{r MLE}
#| echo: false
#| message: false
#| warning: false
A.hat        = solve(t(X)%*%X)%*%t(X)%*%Y                
Sigma.hat    = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y) 
```
The author randomly draws matrices $A$ and $\Sigma$ from the posterior distribution,  which is specified as a Normal Inverse Wishart distribution as described earlier. Following this, the sign restriction is applied to the main diagonal of matrix $B_{0}$, ensuring positive signs.

```{r prior distribution}
#| echo: false
#| message: false
#| warning: false

kappa.1     = 0.02^2
kappa.2     = 100
A.prior     = matrix(0,nrow(A.hat),ncol(A.hat))
A.prior[2:3,] = diag(N)
V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
S.prior     = diag(diag(Sigma.hat))
nu.prior    = N+1
```



```{r Normal-Inverse wishart posterior}
#| echo: false
#| message: false
#| warning: false
V.bar.inv   = t(X)%*%X + diag(1/diag(V.prior))
V.bar       = solve(V.bar.inv)
A.bar       = V.bar%*%(t(X)%*%Y + diag(1/diag(V.prior))%*%A.prior)
nu.bar      = nrow(Y) + nu.prior
S.bar       = S.prior + t(Y)%*%Y + t(A.prior)%*%diag(1/diag(V.prior))%*%A.prior - t(A.bar)%*%V.bar.inv%*%A.bar
S.bar.inv   = solve(S.bar)
```


```{r posterior draws }
#| echo: false
#| message: false
#| warning: false

# Sampling from normal-inverse Wishart distribution
S = 1000 # No. of draws
Sigma.posterior   = rWishart(S, df=nu.bar, Sigma=S.bar.inv) # whole random draw of Sigma
Sigma.posterior   = apply(Sigma.posterior,3,solve)
Sigma.posterior   = array(Sigma.posterior,c(N,N,S))
A.posterior       = array(rnorm(prod(c(dim(A.bar),S))),c(dim(A.bar),S)) # whole random draw of A
B0.tilde          = array(NA,c(N,N,S))
L                 = t(chol(V.bar))
Bplus.tilde       = array(NA,c(N,K,S))

for (s in 1:S){
  cholSigma.s     = chol(Sigma.posterior[,,s])
  B0.tilde[,,s]   = solve(t(cholSigma.s)) 
  A.posterior[,,s]= A.bar + L%*%A.posterior[,,s]%*%cholSigma.s # for each random draw of A
  Bplus.tilde[,,s]  <- B0.tilde[,,s]%*%t(A.posterior[,,s]) 
}

```


```{r Restriction on B0}
#| echo: false
#| message: false
#| warning: false
restriction = diag(c(1,1)) # As we want B0 closely to the identity matrix

 # Storage matrix
i.vec <- c()
Q.store   = array(NA,c(N,N,S))
B0.store = array(NA,c(N,N,S))
Bplus.store = array(NA,c(N,K,S))
#A.store = array (NA,c(K,N,S))
#Sigma.store = array(NA,c(N,N,S))

for (s in 1:S) {
  #A     <- A.posterior[,,s]
  #Sigma <- Sigma.posterior[,,s]
  B0.tilde1      = B0.tilde[,,s]
  Bplus.tilde1   = Bplus.tilde[,,s]
  
  sign.restrictions.do.not.hold = TRUE
  i=1
  
  while (sign.restrictions.do.not.hold){
    X           = matrix(rnorm(N*N),N,N)
    QR          = qr(X, tol = 1e-10)
    Q           = qr.Q(QR,complete=TRUE)
    R           = qr.R(QR,complete=TRUE)
    Q           = t(Q %*% diag(sign(diag(R))))
    B0          = Q%*%B0.tilde1
    Bplus       = Q%*%Bplus.tilde1
    B0.inv      = solve(B0)
    check       = all(diag(B0)>0)
    if (check==1){sign.restrictions.do.not.hold=FALSE}
    i=i+1
  }
    i.vec <- c(i.vec, i)
    Q.store[,,s] <- Q
    B0.store[,,s] <- B0
    B0.mean <- apply(B0.store,1:2,mean)
    Bplus.store[,,s] <- Bplus
    Bplus.mean <- apply(Bplus.store,1:2,mean)
}
```

Table 5 displays that the mean values of the main diagonal elements in matrix $B_{0}$ are positive, providing evidence for the validity of the imposed restriction.

```{r B0 mean}
B0_df <- as.data.frame(B0.mean)
colnames(B0_df) <- c("C1", "C2")
rownames(B0_df) <- c( "R1", "R2")
knitr::kable(B0_df, caption = "Table5: Mean of the B0 matrix (Baseline model)")
```
In Table 6, the values in the first column, which represent the constant term in the equation, are close to zero.


```{r Bplus mean}
Bplus_df <- as.data.frame(Bplus.mean)
colnames(Bplus_df) <- c("C1","C2", "C3")
rownames(Bplus_df) <- c( "R1", "R2")
knitr::kable(Bplus_df, caption = "Table6: Mean of the B+ matrix (Baseline model)")
```



## The extension model
### Estimation algorithm
Suppose the author aims to enhance the flexibility of the prior distribution for gamma ($\gamma$) by modeling it in the following manner.

```{=tex}
\begin{align}
\color{Grey}P(A,\Sigma\color{Black}|\gamma) &= \color{Grey}P(A|\Sigma) P(\Sigma\color{Black}|\gamma) \\
\color{Grey}A|\Sigma &\sim \color{Grey}\mathcal{MN}_{K \times N} \color{Grey}(\underline{A}, \Sigma , \underline{V}) \\
\color{Black}\Sigma|\gamma &\sim \mathcal{IW}_{N}(\gamma I_{N},\underline{\nu}) \\
\gamma &\sim \mathcal{IG2} (\underline{S},\underline{\nu})
\end{align}
```

Thus, the the posterior distribution as follows: 

```{=tex}
\begin{align}
P(A,\Sigma|Y,X,\gamma) \propto & L(A,\Sigma|Y,X,\gamma)P(A,\Sigma) \\
P(A,\Sigma|Y,X,\gamma) \propto & L(A,\Sigma|Y,X,\gamma)P(A|\Sigma) P(\Sigma|\gamma) \\
P(A,\Sigma|Y,X,\gamma) \propto 
& det(\Sigma)^{-\frac{T}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\hat{A})'X'X(A-\hat{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(Y-X\hat{A})'(Y-X\hat{A}) \right] \right\} \\
&det(\Sigma)^{-\frac{N+K+\underline{\nu}+1}{2}} 
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}(A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \right] \right\}
exp \left\{-\frac{1}{2} tr \left[ \Sigma^{-1}\gamma I_{N} \right] \right\} \\
P(A,\Sigma|Y,X,\gamma) \propto 
& det(\Sigma)^{-\frac{T+N+K+\underline{\nu}+1}{2}}
exp \left\{-\frac{1}{2} tr \left[
\Sigma^{-1}\left[(A-\hat{A})'X'X(A-\hat{A}) + (A-\underline{A})'\underline{V}^{-1}(A-\underline{A}) \\
+(Y-X\hat{A})'(Y-X\hat{A})+\gamma I_{N}\right] \right] \right\} \\

\end{align}
```

The distribution represented in the form of Normal Inverse Wishart is characterized by the following parameters:
```{=tex}
\begin{align}

\overline{V} &= (X'X+ \underline{V}^{-1})^{-1} \\ 
\overline{A} &= \overline{V}(X'Y+\underline{V}^{-1} \underline{A})\\
\overline{\nu} &= T+\underline{\nu}\\
\overline{S} &= \gamma I_{N}+Y'Y + \underline{A}'\underline{V}^{-1}\underline{A} - 
\overline{A}'\overline{V}^{-1}\overline{A} \\

\end{align}
```

<br>
The posterior distribution of $\gamma$ is also needed as it serves as a crucial component for the sampling procedure.

```{=tex}
\begin{align}
P(\gamma|A,\Sigma,Y,X) \propto & L(Y|\Sigma,X,A)P(\gamma)P(A,\Sigma)P(\Sigma|\gamma) \\
\propto & P(\gamma)P(\Sigma|\gamma) \\

= & \gamma^{-\frac{\underline{\nu}+2}{2}}\:exp\left\{{-\frac{1}{2}\frac{\underline{S}}{\lambda}} \right\} \:det(\gamma I_{N})^{\frac{\underline{\nu}}{2}}\:det(\Sigma)^{-\frac{\underline{\nu}+N+1}{2}}\:exp\left\{{-\frac{1}{2}tr[\Sigma^{-1}\gamma I_{N}]} \right\} \\

= &\gamma^{-\frac{\underline{\nu}+2}{2}}\:exp\left\{{-\frac{1}{2}\frac{\underline{S}}{\gamma}} \right\}\:\gamma^{-\frac{N\underline{\nu}}{2}}\:det(\Sigma)^{\frac{\underline{\nu}+N+1}{2}}\:exp\left\{{-\frac{1}{2}\gamma \:tr[\Sigma^{-1}]} \right\} \\

\propto &\gamma^{\frac{N\underline{\nu}-\underline{\nu}}{2}-1} \: 
exp\left\{{-\frac{1}{2}}[\gamma\:tr[\Sigma^{-1}]+{\frac{\underline{S}}{\gamma}}]\right\} \\

\end{align}
```

The following distribution conforms to the Generalized Inverse Gaussian (GIG) distribution, with the parameters as follows:

```{=tex}
\begin{align}
\lambda = {\frac{N\underline{\nu}-\underline{\nu}}{2}} \\

\\ \chi = \underline{S}\\

\\ \psi = tr[\Sigma^{-1}]
\end{align}
```

The Gibbs Sampler is used to get the posterior draws of the extended model

For S = 1,000 iterations 

Initialize $\gamma$ at $\gamma^{(0)}$

At each iteration:

1) Draw $\Sigma^{(s)} \sim P(\Sigma|Y,X,\gamma^{(s-1)})$
2) Draw $A^{(s)}\sim P(A|\Sigma^{(s)},Y,X)$ 
3) Draw $\gamma^{(s)} \sim P(\gamma|Y,X,A^{(s)},\Sigma^{(s)})$


The author then utilizes the random draws from $\Sigma^{(s)}, A^{(s)}, \gamma^{(s)}$ to obtain the posterior draw and create the following B0 and B plus matrices.


```{r}
S.bar1 <- array(0, dim = c(2, 2, S))
S.bar.inv1 <- array(0, dim = c(2, 2, S))
#A.posterior_list <- list()
gamma.store <- numeric(S)

for (i in 2:S) {
  S.bar1[,,i] <- gamma.store[i-1] * diag(2) + t(Y) %*% Y + t(A.prior) %*% diag(1/diag(V.prior)) %*% A.prior - t(A.bar) %*% V.bar.inv %*% A.bar
  S.bar.inv1[,,i] <- solve(S.bar1[,,i])
  
  Sigma.posterior <- rWishart(S, df = nu.bar, Sigma = S.bar.inv1[,,i])
  Sigma.posterior <- apply(Sigma.posterior, 3, solve)
  Sigma.posterior <- array(Sigma.posterior, c(N, N, S))
  
  cholSigma.s <- chol(Sigma.posterior[,,i])
  A.posterior[,,i] <- A.bar + L %*% A.posterior[,,i] %*% cholSigma.s
  
  lambda <- ((nu.prior * N) - nu.prior) / 2
  chi <- S.prior
  psi <- sum(diag(solve(Sigma.posterior[,,i])))
  gamma.store[i] <- GIGrvg::rgig(n = 1, lambda, chi, psi)
  
  B0.tilde[,,i]   = solve(t(cholSigma.s)) 
  Bplus.tilde[,,i]  <- B0.tilde[,,i]%*%t(A.posterior[,,i]) 
  
}
```

```{r}
# collect sigma, A, gamma and other variables used to compute B0 and B+ (ignore first element)

A.posterior = A.posterior[,,2:S]
gamma.store = gamma.store[2:S]
B0.tilde = B0.tilde[,,2:S]
Bplus.tilde = Bplus.tilde[,,2:S]
Sigma.posterior = Sigma.posterior[,,2:S]
```


```{r Restriction on B0 extension}
#| echo: false
#| message: false
#| warning: false

 # Storage matrix
i.vec <- c()
Q.store   = array(NA,c(N,N,(S-1)))
B0.store = array(NA,c(N,N,(S-1)))
Bplus.store = array(NA,c(N,K,(S-1)))

for (s in 1:(S-1)) {
  B0.tilde2      = B0.tilde[,,s]
  Bplus.tilde2   = Bplus.tilde[,,s]
  
  sign.restrictions.do.not.hold = TRUE
  i=1
  
  while (sign.restrictions.do.not.hold){
    X           = matrix(rnorm(N*N),N,N)
    QR          = qr(X, tol = 1e-10)
    Q           = qr.Q(QR,complete=TRUE)
    R           = qr.R(QR,complete=TRUE)
    Q           = t(Q %*% diag(sign(diag(R))))
    B0          = Q%*%B0.tilde2
    Bplus       = Q%*%Bplus.tilde2
    B0.inv      = solve(B0)
    check       = all(diag(B0)>0)
    if (check==1){sign.restrictions.do.not.hold=FALSE}
    i=i+1
  }
    i.vec <- c(i.vec, i)
    Q.store[,,s] <- Q
    B0.store[,,s] <- B0
    B0.mean <- apply(B0.store,1:2,mean)
    Bplus.store[,,s] <- Bplus
    Bplus.mean <- apply(Bplus.store,1:2,mean)
}
```

### Data simulation

Table 7 indicates that the mean value of the main diagonal of $B_{0}$ remains positive, exhibiting a similar behavior to that of the basic model.

```{r B0 mean for extension}
B0_df <- as.data.frame(B0.mean)
colnames(B0_df) <- c("C1", "C2")
rownames(B0_df) <- c( "R1", "R2")
knitr::kable(B0_df, caption = "Table7: Mean of the B0 matrix (extension model)")
```

```{r Bplus mean for extension}
Bplus_df <- as.data.frame(Bplus.mean)
colnames(Bplus_df) <- c("C1","C2", "C3")
rownames(Bplus_df) <- c( "R1", "R2")
knitr::kable(Bplus_df, caption = "Table8: Mean of the B+ matrix (extension model)")
```



## References {.unnumbered}
